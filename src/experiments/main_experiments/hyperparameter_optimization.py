#!/usr/bin/env python3
"""
è¶…å‚æ•°ä¼˜åŒ–å™¨ - MAMA ç³»ç»Ÿå­¦æœ¯å®éªŒ
è§£å†³è®ºæ–‡ä¸­è¶…å‚æ•°é€‰æ‹©çš„éšæ„æ€§é—®é¢˜ï¼Œè¿›è¡Œç½‘æ ¼æœç´¢å’Œæ•æ„Ÿæ€§åˆ†æ
"""

import json
import numpy as np
import time
import logging
from typing import Dict, List, Any, Tuple, Optional, Iterator
from itertools import product
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# å¯¼å…¥æ¨¡å‹å’Œè¯„ä¼°å™¨
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.base_model import ModelConfig
from models.mama_full import MAMAFull
from evaluation.standard_evaluator import StandardEvaluator

logger = logging.getLogger(__name__)

class HyperparameterOptimizer:
    """è¶…å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, random_seed: int = 42):
        """
        åˆå§‹åŒ–è¶…å‚æ•°ä¼˜åŒ–å™¨
        
        Args:
            random_seed: éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
        """
        self.random_seed = random_seed
        np.random.seed(random_seed)
        
        # è¯„ä¼°å™¨
        self.evaluator = StandardEvaluator(random_seed)
        
        # ä¼˜åŒ–å†å²
        self.optimization_history = []
        
        # æœ€ä½³é…ç½®
        self.best_config = None
        self.best_score = -np.inf
        
        logger.info("âœ… è¶…å‚æ•°ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def define_search_space(self) -> Dict[str, List[float]]:
        """
        å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
        
        Returns:
            è¶…å‚æ•°æœç´¢ç©ºé—´å­—å…¸
        """
        search_space = {
            # MAMAæ ¸å¿ƒæƒé‡å‚æ•°
            'alpha': [0.5, 0.6, 0.7, 0.8, 0.9],  # SBERTç›¸ä¼¼åº¦æƒé‡
            'beta': [0.1, 0.2, 0.3, 0.4, 0.5],   # ä¿¡ä»»åˆ†æ•°æƒé‡
            'gamma': [0.05, 0.1, 0.15, 0.2, 0.25], # å†å²è¡¨ç°æƒé‡
            
            # ä¿¡ä»»åˆ†æ•°æƒé‡åˆ†é…
            'trust_reliability': [0.2, 0.25, 0.3],
            'trust_accuracy': [0.2, 0.25, 0.3],
            'trust_consistency': [0.15, 0.2, 0.25],
            'trust_transparency': [0.1, 0.15, 0.2],
            'trust_robustness': [0.1, 0.15, 0.2],
            
            # ç³»ç»Ÿå‚æ•°
            'max_agents': [2, 3, 4, 5],
            'trust_threshold': [0.3, 0.4, 0.5, 0.6, 0.7]
        }
        
        return search_space
    
    def grid_search(self, test_data: List[Dict[str, Any]], 
                   max_combinations: int = 50,
                   validation_split: float = 0.3) -> Dict[str, Any]:
        """
        ç½‘æ ¼æœç´¢æœ€ä¼˜è¶…å‚æ•°
        
        Args:
            test_data: æµ‹è¯•æ•°æ®
            max_combinations: æœ€å¤§æœç´¢ç»„åˆæ•°
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            
        Returns:
            ä¼˜åŒ–ç»“æœ
        """
        logger.info(f"ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢è¶…å‚æ•°ä¼˜åŒ–")
        logger.info(f"ğŸ“Š æµ‹è¯•æ•°æ®: {len(test_data)} æ¡")
        logger.info(f"ğŸ“Š æœ€å¤§æœç´¢ç»„åˆ: {max_combinations}")
        
        # åˆ†å‰²éªŒè¯é›†
        val_size = int(len(test_data) * validation_split)
        validation_data = test_data[:val_size]
        remaining_data = test_data[val_size:]
        
        # è·å–æœç´¢ç©ºé—´
        search_space = self.define_search_space()
        
        # ç”Ÿæˆè¶…å‚æ•°ç»„åˆ
        param_combinations = self._generate_param_combinations(search_space, max_combinations)
        
        logger.info(f"ğŸ¯ å®é™…æœç´¢ç»„åˆæ•°: {len(param_combinations)}")
        
        # æ‰§è¡Œç½‘æ ¼æœç´¢
        search_results = []
        
        for i, params in enumerate(param_combinations):
            logger.info(f"ğŸ”„ æµ‹è¯•ç»„åˆ {i+1}/{len(param_combinations)}: {params}")
            
            try:
                # åˆ›å»ºæ¨¡å‹é…ç½®
                config = self._create_model_config(params)
                
                # è¯„ä¼°æ¨¡å‹
                result = self._evaluate_config(config, validation_data)
                
                # è®°å½•ç»“æœ
                search_results.append({
                    'combination_id': i + 1,
                    'parameters': params,
                    'metrics': result['metrics'],
                    'evaluation_time': result['evaluation_time']
                })
                
                # æ›´æ–°æœ€ä½³é…ç½®
                score = result['metrics']['MRR']  # ä½¿ç”¨MRRä½œä¸ºä¸»è¦æŒ‡æ ‡
                if score > self.best_score:
                    self.best_score = score
                    self.best_config = params
                    logger.info(f"ğŸŒŸ å‘ç°æ›´å¥½é…ç½®! MRR = {score:.4f}")
                
            except Exception as e:
                logger.error(f"âŒ è¯„ä¼°ç»„åˆ {i+1} å¤±è´¥: {e}")
                continue
        
        # åˆ†æç»“æœ
        optimization_result = self._analyze_grid_search_results(search_results)
        
        # åœ¨å‰©ä½™æ•°æ®ä¸ŠéªŒè¯æœ€ä½³é…ç½®
        best_validation = self._validate_best_config(remaining_data)
        optimization_result['final_validation'] = best_validation
        
        logger.info(f"âœ… ç½‘æ ¼æœç´¢å®Œæˆ!")
        logger.info(f"ğŸ† æœ€ä½³é…ç½® MRR: {self.best_score:.4f}")
        
        return optimization_result
    
    def _generate_param_combinations(self, search_space: Dict[str, List[float]], 
                                   max_combinations: int) -> List[Dict[str, float]]:
        """ç”Ÿæˆå‚æ•°ç»„åˆ"""
        # è®¡ç®—æ‰€æœ‰å¯èƒ½çš„ç»„åˆæ•°
        total_combinations = 1
        for values in search_space.values():
            total_combinations *= len(values)
        
        logger.info(f"ğŸ“Š ç†è®ºç»„åˆæ•°: {total_combinations}")
        
        if total_combinations <= max_combinations:
            # å¦‚æœç»„åˆæ•°ä¸å¤šï¼Œä½¿ç”¨å®Œæ•´ç½‘æ ¼æœç´¢
            param_names = list(search_space.keys())
            param_values = list(search_space.values())
            
            combinations = []
            for combination in product(*param_values):
                param_dict = dict(zip(param_names, combination))
                
                # æ£€æŸ¥æƒé‡çº¦æŸ
                if self._is_valid_combination(param_dict):
                    combinations.append(param_dict)
            
            return combinations
        else:
            # å¦‚æœç»„åˆæ•°å¤ªå¤šï¼Œä½¿ç”¨éšæœºé‡‡æ ·
            logger.info(f"ğŸ² ä½¿ç”¨éšæœºé‡‡æ · {max_combinations} ä¸ªç»„åˆ")
            
            combinations = []
            attempts = 0
            max_attempts = max_combinations * 10
            
            while len(combinations) < max_combinations and attempts < max_attempts:
                param_dict = {}
                for param_name, values in search_space.items():
                    param_dict[param_name] = np.random.choice(values)
                
                if self._is_valid_combination(param_dict):
                    combinations.append(param_dict)
                
                attempts += 1
            
            return combinations
    
    def _is_valid_combination(self, params: Dict[str, float]) -> bool:
        """æ£€æŸ¥å‚æ•°ç»„åˆæ˜¯å¦æœ‰æ•ˆ"""
        # æ£€æŸ¥ä¸»è¦æƒé‡å’Œæ˜¯å¦æ¥è¿‘1.0
        main_weights_sum = params.get('alpha', 0.7) + params.get('beta', 0.2) + params.get('gamma', 0.1)
        if not (0.95 <= main_weights_sum <= 1.05):
            return False
        
        # æ£€æŸ¥ä¿¡ä»»æƒé‡å’Œæ˜¯å¦æ¥è¿‘1.0
        trust_weights_sum = (
            params.get('trust_reliability', 0.25) +
            params.get('trust_accuracy', 0.25) +
            params.get('trust_consistency', 0.2) +
            params.get('trust_transparency', 0.15) +
            params.get('trust_robustness', 0.15)
        )
        if not (0.95 <= trust_weights_sum <= 1.05):
            return False
        
        return True
    
    def _create_model_config(self, params: Dict[str, float]) -> ModelConfig:
        """æ ¹æ®å‚æ•°åˆ›å»ºæ¨¡å‹é…ç½®"""
        # å½’ä¸€åŒ–ä¸»è¦æƒé‡
        total_main = params.get('alpha', 0.7) + params.get('beta', 0.2) + params.get('gamma', 0.1)
        alpha = params.get('alpha', 0.7) / total_main
        beta = params.get('beta', 0.2) / total_main
        gamma = params.get('gamma', 0.1) / total_main
        
        # å½’ä¸€åŒ–ä¿¡ä»»æƒé‡
        total_trust = (
            params.get('trust_reliability', 0.25) +
            params.get('trust_accuracy', 0.25) +
            params.get('trust_consistency', 0.2) +
            params.get('trust_transparency', 0.15) +
            params.get('trust_robustness', 0.15)
        )
        
        trust_weights = {
            'reliability': params.get('trust_reliability', 0.25) / total_trust,
            'accuracy': params.get('trust_accuracy', 0.25) / total_trust,
            'consistency': params.get('trust_consistency', 0.2) / total_trust,
            'transparency': params.get('trust_transparency', 0.15) / total_trust,
            'robustness': params.get('trust_robustness', 0.15) / total_trust
        }
        
        config = ModelConfig(
            alpha=alpha,
            beta=beta,
            gamma=gamma,
            trust_weights=trust_weights,
            max_agents=int(params.get('max_agents', 3)),
            trust_threshold=params.get('trust_threshold', 0.5),
            random_seed=self.random_seed
        )
        
        return config
    
    def _evaluate_config(self, config: ModelConfig, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªé…ç½®"""
        start_time = time.time()
        
        # åˆ›å»ºæ¨¡å‹
        model = MAMAFull(config)
        
        # è¯„ä¼°æ¨¡å‹
        result = self.evaluator.evaluate_model(model, test_data, f"MAMA_Config_{len(self.optimization_history)}")
        
        evaluation_time = time.time() - start_time
        
        return {
            'metrics': result['metrics'],
            'evaluation_time': evaluation_time
        }
    
    def _analyze_grid_search_results(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æç½‘æ ¼æœç´¢ç»“æœ"""
        if not search_results:
            return {}
        
        # æå–æŒ‡æ ‡æ•°æ®
        mrr_scores = [result['metrics']['MRR'] for result in search_results]
        ndcg_scores = [result['metrics']['NDCG@5'] for result in search_results]
        art_scores = [result['metrics']['ART'] for result in search_results]
        
        # ç»Ÿè®¡åˆ†æ
        analysis = {
            'search_summary': {
                'total_combinations': len(search_results),
                'best_mrr': max(mrr_scores),
                'best_ndcg': max(ndcg_scores),
                'best_art': min(art_scores),
                'avg_mrr': np.mean(mrr_scores),
                'std_mrr': np.std(mrr_scores),
                'best_configuration': self.best_config
            },
            'performance_distribution': {
                'mrr_percentiles': {
                    '25th': np.percentile(mrr_scores, 25),
                    '50th': np.percentile(mrr_scores, 50),
                    '75th': np.percentile(mrr_scores, 75),
                    '90th': np.percentile(mrr_scores, 90)
                },
                'ndcg_percentiles': {
                    '25th': np.percentile(ndcg_scores, 25),
                    '50th': np.percentile(ndcg_scores, 50),
                    '75th': np.percentile(ndcg_scores, 75),
                    '90th': np.percentile(ndcg_scores, 90)
                }
            },
            'detailed_results': search_results
        }
        
        # å‚æ•°æ•æ„Ÿæ€§åˆ†æ
        sensitivity_analysis = self._parameter_sensitivity_analysis(search_results)
        analysis['sensitivity_analysis'] = sensitivity_analysis
        
        # ä¿å­˜ç»“æœ
        self.optimization_history.append(analysis)
        
        return analysis
    
    def _parameter_sensitivity_analysis(self, search_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å‚æ•°æ•æ„Ÿæ€§åˆ†æ"""
        sensitivity = {}
        
        # åˆ†ææ¯ä¸ªå‚æ•°å¯¹æ€§èƒ½çš„å½±å“
        for param_name in ['alpha', 'beta', 'gamma', 'max_agents', 'trust_threshold']:
            param_impact = self._analyze_parameter_impact(search_results, param_name)
            sensitivity[param_name] = param_impact
        
        return sensitivity
    
    def _analyze_parameter_impact(self, search_results: List[Dict[str, Any]], 
                                 param_name: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå‚æ•°çš„å½±å“"""
        param_values = []
        mrr_scores = []
        
        for result in search_results:
            if param_name in result['parameters']:
                param_values.append(result['parameters'][param_name])
                mrr_scores.append(result['metrics']['MRR'])
        
        if not param_values:
            return {}
        
        # è®¡ç®—ç›¸å…³æ€§
        correlation = np.corrcoef(param_values, mrr_scores)[0, 1] if len(param_values) > 1 else 0.0
        
        # æŒ‰å‚æ•°å€¼åˆ†ç»„è®¡ç®—å¹³å‡æ€§èƒ½
        unique_values = sorted(list(set(param_values)))
        avg_performance = {}
        
        for value in unique_values:
            scores = [mrr_scores[i] for i, pv in enumerate(param_values) if pv == value]
            avg_performance[value] = {
                'mean_mrr': np.mean(scores),
                'std_mrr': np.std(scores),
                'count': len(scores)
            }
        
        return {
            'correlation_with_mrr': correlation,
            'value_impact': avg_performance,
            'sensitivity_level': 'high' if abs(correlation) > 0.5 else 'medium' if abs(correlation) > 0.3 else 'low'
        }
    
    def _validate_best_config(self, test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸ŠéªŒè¯æœ€ä½³é…ç½®"""
        if not self.best_config:
            return {}
        
        logger.info("ğŸ§ª åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸ŠéªŒè¯æœ€ä½³é…ç½®...")
        
        # åˆ›å»ºæœ€ä½³é…ç½®çš„æ¨¡å‹
        best_model_config = self._create_model_config(self.best_config)
        
        # è¯„ä¼°æœ€ä½³æ¨¡å‹
        result = self._evaluate_config(best_model_config, test_data)
        
        logger.info(f"âœ… æœ€ä½³é…ç½®éªŒè¯å®Œæˆ: MRR={result['metrics']['MRR']:.4f}")
        
        return {
            'best_config': self.best_config,
            'validation_metrics': result['metrics'],
            'validation_note': 'Evaluated on independent test set'
        }
    
    def generate_optimization_report(self, output_dir: str = "results/hyperparameter_optimization"):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        if not self.optimization_history:
            logger.warning("âš ï¸ æ²¡æœ‰ä¼˜åŒ–å†å²æ•°æ®")
            return
        
        latest_analysis = self.optimization_history[-1]
        
        # 1. ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = Path(output_dir) / "optimization_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(latest_analysis, f, ensure_ascii=False, indent=2, default=str)
        
        # 2. ç”Ÿæˆå¯è§†åŒ–
        self._create_optimization_visualizations(latest_analysis, output_dir)
        
        # 3. ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._create_text_report(latest_analysis, output_dir)
        
        logger.info(f"ğŸ“Š ä¼˜åŒ–æŠ¥å‘Šå·²ç”Ÿæˆ: {output_dir}")
    
    def _create_optimization_visualizations(self, analysis: Dict[str, Any], output_dir: str):
        """åˆ›å»ºä¼˜åŒ–å¯è§†åŒ–å›¾è¡¨"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. æ€§èƒ½åˆ†å¸ƒå›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # MRRåˆ†å¸ƒ
        search_results = analysis['detailed_results']
        mrr_scores = [result['metrics']['MRR'] for result in search_results]
        
        axes[0, 0].hist(mrr_scores, bins=20, alpha=0.7, color='blue')
        axes[0, 0].axvline(analysis['search_summary']['best_mrr'], color='red', linestyle='--', label='Best MRR')
        axes[0, 0].set_title('MRR Score Distribution')
        axes[0, 0].set_xlabel('MRR Score')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].legend()
        
        # NDCGåˆ†å¸ƒ
        ndcg_scores = [result['metrics']['NDCG@5'] for result in search_results]
        axes[0, 1].hist(ndcg_scores, bins=20, alpha=0.7, color='green')
        axes[0, 1].axvline(analysis['search_summary']['best_ndcg'], color='red', linestyle='--', label='Best NDCG@5')
        axes[0, 1].set_title('NDCG@5 Score Distribution')
        axes[0, 1].set_xlabel('NDCG@5 Score')
        axes[0, 1].set_ylabel('Frequency')
        axes[0, 1].legend()
        
        # å‚æ•°æ•æ„Ÿæ€§çƒ­åŠ›å›¾
        if 'sensitivity_analysis' in analysis:
            sensitivity = analysis['sensitivity_analysis']
            param_names = list(sensitivity.keys())
            correlations = [sensitivity[param].get('correlation_with_mrr', 0) for param in param_names]
            
            axes[1, 0].barh(param_names, correlations)
            axes[1, 0].set_title('Parameter Sensitivity (Correlation with MRR)')
            axes[1, 0].set_xlabel('Correlation Coefficient')
            
        # æ€§èƒ½vsè¯„ä¼°æ—¶é—´
        eval_times = [result['evaluation_time'] for result in search_results]
        axes[1, 1].scatter(eval_times, mrr_scores, alpha=0.6)
        axes[1, 1].set_title('Performance vs Evaluation Time')
        axes[1, 1].set_xlabel('Evaluation Time (seconds)')
        axes[1, 1].set_ylabel('MRR Score')
        
        plt.tight_layout()
        plt.savefig(Path(output_dir) / "optimization_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("ğŸ“ˆ ä¼˜åŒ–å¯è§†åŒ–å›¾è¡¨å·²ç”Ÿæˆ")
    
    def _create_text_report(self, analysis: Dict[str, Any], output_dir: str):
        """åˆ›å»ºæ–‡æœ¬æŠ¥å‘Š"""
        report_file = Path(output_dir) / "optimization_report.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# MAMAç³»ç»Ÿè¶…å‚æ•°ä¼˜åŒ–æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # æœç´¢æ‘˜è¦
            f.write("## æœç´¢æ‘˜è¦\n\n")
            summary = analysis['search_summary']
            f.write(f"- æœç´¢ç»„åˆæ•°: {summary['total_combinations']}\n")
            f.write(f"- æœ€ä½³MRR: {summary['best_mrr']:.4f}\n")
            f.write(f"- æœ€ä½³NDCG@5: {summary['best_ndcg']:.4f}\n")
            f.write(f"- å¹³å‡MRR: {summary['avg_mrr']:.4f} Â± {summary['std_mrr']:.4f}\n\n")
            
            # æœ€ä½³é…ç½®
            f.write("## æœ€ä½³é…ç½®\n\n")
            best_config = summary['best_configuration']
            f.write("```json\n")
            f.write(json.dumps(best_config, indent=2, ensure_ascii=False))
            f.write("\n```\n\n")
            
            # æ•æ„Ÿæ€§åˆ†æ
            if 'sensitivity_analysis' in analysis:
                f.write("## å‚æ•°æ•æ„Ÿæ€§åˆ†æ\n\n")
                sensitivity = analysis['sensitivity_analysis']
                
                for param_name, param_analysis in sensitivity.items():
                    correlation = param_analysis.get('correlation_with_mrr', 0)
                    sensitivity_level = param_analysis.get('sensitivity_level', 'unknown')
                    
                    f.write(f"### {param_name}\n")
                    f.write(f"- ä¸MRRç›¸å…³æ€§: {correlation:.3f}\n")
                    f.write(f"- æ•æ„Ÿæ€§ç­‰çº§: {sensitivity_level}\n\n")
            
            # å­¦æœ¯æ„ä¹‰
            f.write("## å­¦æœ¯æ„ä¹‰\n\n")
            f.write("æœ¬ä¼˜åŒ–è¿‡ç¨‹è§£å†³äº†ä»¥ä¸‹å­¦æœ¯é—®é¢˜:\n\n")
            f.write("1. **è¶…å‚æ•°é€‰æ‹©çš„ç§‘å­¦æ€§**: é€šè¿‡ç½‘æ ¼æœç´¢è€Œéç»éªŒè®¾å®šè¶…å‚æ•°\n")
            f.write("2. **æ¨¡å‹é²æ£’æ€§éªŒè¯**: æ•æ„Ÿæ€§åˆ†æè¯æ˜æ¨¡å‹å¯¹å‚æ•°å˜åŒ–çš„ç¨³å®šæ€§\n")
            f.write("3. **æ€§èƒ½ä¸Šé™æ¢ç´¢**: ç³»ç»Ÿæ€§æœç´¢æ‰¾åˆ°æœ€ä¼˜é…ç½®\n")
            f.write("4. **å®éªŒå¯å¤ç°æ€§**: è¯¦ç»†è®°å½•æ‰€æœ‰å‚æ•°ç»„åˆå’Œç»“æœ\n\n")
        
        logger.info("ğŸ“ æ–‡æœ¬æŠ¥å‘Šå·²ç”Ÿæˆ")
    
    def load_optimization_results(self, results_file: str) -> Dict[str, Any]:
        """åŠ è½½ä¹‹å‰çš„ä¼˜åŒ–ç»“æœ"""
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            logger.info(f"âœ… åŠ è½½ä¼˜åŒ–ç»“æœ: {results_file}")
            return results
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ä¼˜åŒ–ç»“æœå¤±è´¥: {e}")
            return {} 