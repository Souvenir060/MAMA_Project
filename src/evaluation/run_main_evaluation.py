#!/usr/bin/env python3
"""
MAMA Framework - Main Evaluation Script
Runs comprehensive evaluation of all models
"""

import json
import logging
import os
import time
import numpy as np
import pandas as pd
import argparse
import sys
import pickle
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# Add parent directory to path
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)
sys.path.append(current_dir)

from models.mama_full import MAMAFull
from models.mama_no_trust import MAMANoTrust
from models.single_agent_system import SingleAgentSystemModel
from models.traditional_ranking import TraditionalRanking
from core.evaluation_metrics import calculate_mrr, calculate_ndcg_at_k
from evaluation.standard_evaluator import StandardEvaluator

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Set random seed for reproducibility
np.random.seed(42)

# Global cache for embeddings and models
EMBEDDING_CACHE = {}
MODEL_CACHE = {}

def setup_model_cache():
    """Pre-initialize and cache models to avoid repeated initialization"""
    global MODEL_CACHE
    
    logger.info("üöÄ Pre-initializing models for performance optimization...")
    
    try:
        # Initialize all models once
        MODEL_CACHE['MAMA_Full'] = MAMAFull()
        MODEL_CACHE['MAMA_NoTrust'] = MAMANoTrust()
        MODEL_CACHE['SingleAgent'] = SingleAgentSystemModel()
        MODEL_CACHE['Traditional'] = TraditionalRanking()
        
        logger.info("‚úÖ All models pre-initialized and cached")
        
        # Pre-load SBERT model if not already loaded
        if hasattr(MODEL_CACHE['MAMA_Full'], 'sbert_engine'):
            logger.info("‚úÖ SBERT model pre-loaded")
            
    except Exception as e:
        logger.error(f"‚ùå Error initializing models: {e}")
        MODEL_CACHE = {}

def get_cached_model(model_name: str):
    """Get cached model instance"""
    return MODEL_CACHE.get(model_name)
        
def load_test_dataset() -> List[Dict[str, Any]]:
    """
    Load test dataset from multiple sources
    
    Returns:
        List of test query dictionaries
    """
    test_data = []
    
    # Try to load from multiple possible sources - CORRECTED PATHS
    possible_sources = [
        'src/data/test_queries.json',  # CRITICAL FIX: Use the correct file with ground truth!
        'data/test_queries.json',
        'data/test_queries_150.json', 
        '../data/test_queries_150.json',
        'data/standard_dataset.json'
    ]
    
    for source_path in possible_sources:
        try:
            if os.path.exists(source_path):
                with open(source_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Handle different data structures
                if isinstance(data, dict) and 'queries' in data:
                    queries = data['queries']
                elif isinstance(data, list):
                    queries = data
                else:
                    logger.warning(f"‚ö†Ô∏è Unexpected data structure in {source_path}")
                    continue
                
                # Process and standardize each query
                for query in queries:
                    processed_query = standardize_query_format(query)
                    if processed_query:
                        test_data.append(processed_query)
                
                logger.info(f"‚úÖ Loaded {len(queries)} queries from {source_path}")
                break  # Use first successful source
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load from {source_path}: {e}")
            continue
    
    if not test_data:
        logger.error("‚ùå No test data loaded from any source!")
        raise FileNotFoundError("Could not load test dataset")
    
    logger.info(f"üìä Total test queries loaded: {len(test_data)}")
    return test_data

def standardize_query_format(query: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Standardize query format to ensure consistent field names
    
    Args:
        query: Raw query dictionary
        
    Returns:
        Standardized query dictionary
    """
    try:
        standardized = query.copy()
        
        # Key field mappings to fix data interface issues
        field_mappings = {
            'candidate_flights': 'flight_candidates',  # Critical fix!
            'candidates': 'flight_candidates',
            'flights': 'flight_candidates'
        }
        
        # Apply field mappings
        for old_field, new_field in field_mappings.items():
            if old_field in query and new_field not in query:
                standardized[new_field] = query[old_field]
                logger.debug(f"üîÑ Mapped {old_field} ‚Üí {new_field}")
        
        # Ensure required fields exist
        required_fields = ['query_id', 'query_text', 'flight_candidates']
        
        for field in required_fields:
            if field not in standardized:
                if field == 'flight_candidates':
                    # Try to find flight data in other fields
                    flight_data = (standardized.get('candidate_flights') or 
                                 standardized.get('candidates') or 
                                 standardized.get('flights') or [])
                    standardized['flight_candidates'] = flight_data
                elif field == 'query_text':
                    # Generate basic query text if missing
                    params = standardized.get('parameters', {})
                    origin = params.get('origin', 'Unknown')
                    destination = params.get('destination', 'Unknown') 
                    date = params.get('date', 'Unknown')
                    standardized['query_text'] = f"Flight from {origin} to {destination} on {date}"
                elif field == 'query_id':
                    standardized['query_id'] = f"query_{hash(str(query)) % 10000:04d}"
        
        # Validate that we have flight candidates
        if not standardized.get('flight_candidates'):
            logger.warning(f"‚ö†Ô∏è Query {standardized.get('query_id', 'unknown')} has no flight candidates")
            return None
            
        # Ensure flight_candidates have proper flight_id
        flight_candidates = standardized['flight_candidates']
        for i, flight in enumerate(flight_candidates):
            if 'flight_id' not in flight:
                flight['flight_id'] = f"{standardized['query_id']}_flight_{i+1:02d}"
        
        return standardized
        
    except Exception as e:
        logger.error(f"‚ùå Error standardizing query: {e}")
        return None

def optimize_batch_processing(queries: List[Dict[str, Any]], batch_size: int = 10) -> List[List[Dict[str, Any]]]:
    """
    Split queries into batches for more efficient processing
    
    Args:
        queries: List of test queries
        batch_size: Number of queries per batch
    
    Returns:
        List of query batches
    """
    batches = []
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i + batch_size]
        batches.append(batch)
    
    logger.info(f"üì¶ Split {len(queries)} queries into {len(batches)} batches of size {batch_size}")
    return batches
    
def run_model_evaluation(model, queries: List[Dict[str, Any]], 
                                 use_cache: bool = True, verbose: bool = False) -> List[Dict[str, Any]]:
    """
    Run evaluation on a single model
    
    Args:
        model: Model to evaluate
        queries: List of test queries
        use_cache: Whether to use cached results if available
        verbose: Whether to display detailed progress
        
    Returns:
        List of evaluation results for each query
    """
    results = []
    
    # Check for cached results
    cache_file = Path(f'results/cache/{model.model_name.lower().replace(" ", "_")}_results.json')
    if use_cache and cache_file.exists():
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_results = json.load(f)
            logger.info(f"üìÑ Loaded cached results for {model.model_name}")
            return cached_results
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load cached results: {e}")
    
    # Ensure cache directory exists
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    
    total_queries = len(queries)
    start_time_all = time.time()
    
    # Batch processing for better performance
    query_batches = optimize_batch_processing(queries, batch_size=10)
    
    for batch_idx, batch in enumerate(query_batches):
        logger.info(f"üîÑ Processing batch {batch_idx + 1}/{len(query_batches)} ({len(batch)} queries)")
        
        batch_results = []
        
        for i, query in enumerate(batch):
            global_query_idx = batch_idx * 10 + i + 1
            
            if verbose:
                logger.info(f"Processing query {global_query_idx}/{total_queries} with {model.model_name}")
            
            start_time = time.time()
            
            try:
                # Process the query with the model
                result = model.process_query(query)
                processing_time = time.time() - start_time
                
                # üîß CRITICAL FIX: Extract ground truth correctly
                ground_truth_ranking = query.get('ground_truth_ranking', [])
                ground_truth_id = ground_truth_ranking[0] if ground_truth_ranking else ''
                recommendations = result.get('recommendations', [])
                recommendation_ids = [r.get('flight_id', '') for r in recommendations]
                
                # Calculate metrics using correct ground truth format
                mrr = calculate_mrr([{
                    'ground_truth_id': ground_truth_id,
                    'recommendations': recommendation_ids
                }])
                
                ndcg5 = calculate_ndcg_at_k([{
                    'ground_truth_id': ground_truth_id,
                    'recommendations': recommendation_ids
                }], k=5)
                
                # Store result with correct ground truth
                batch_results.append({
                    'query_id': query.get('query_id', f'query_{global_query_idx:03d}'),
                    'ground_truth_id': ground_truth_id,
                    'ground_truth_ranking': ground_truth_ranking,
                    'recommendations': recommendation_ids,
                    'MRR': float(mrr),
                    'NDCG@5': float(ndcg5),
                    'Response_Time': float(processing_time),
                    'model': model.model_name
                })
                
            except Exception as e:
                logger.error(f"‚ùå Error processing query {global_query_idx}: {e}")
                
                # Add failed result
                batch_results.append({
                    'query_id': query.get('query_id', f'query_{global_query_idx:03d}'),
                    'ground_truth_id': query.get('ground_truth_id', ''),
                    'recommendations': [],
                    'MRR': 0.0,
                    'NDCG@5': 0.0,
                    'Response_Time': time.time() - start_time,
                    'model': model.model_name,
                    'error': str(e)
                })
        
        # Add batch results to overall results
        results.extend(batch_results)
        
        # Progress update
        processed_queries = min((batch_idx + 1) * 10, total_queries)
        logger.info(f"‚úÖ Batch {batch_idx + 1} completed: {processed_queries}/{total_queries} queries processed")
    
    total_time = time.time() - start_time_all
    logger.info(f"üèÅ Finished evaluating {model.model_name} on {total_queries} queries in {total_time:.2f}s")
    logger.info(f"‚ö° Average time per query: {total_time/total_queries:.3f}s")
    
    # Cache results
    with open(cache_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    return results
    
def calculate_statistics(results: Dict[str, Any], model_name: str) -> Dict[str, Any]:
    """
    Calculate statistics for a model's results
    
    Args:
        results: Evaluation results from the model
        model_name: Name of the model
        
    Returns:
        Dictionary of statistics with individual metrics for significance testing
    """
    # Extract metrics from results
    metrics = results.get('metrics', {})
    individual_metrics = results.get('individual_metrics', {})
    
    # Create statistics dictionary
    stats = {
        'model': model_name,
        'MRR_mean': metrics.get('MRR', 0.0),
        'MRR_std': metrics.get('MRR_std', 0.0),
        'NDCG@5_mean': metrics.get('NDCG@5', 0.0),
        'NDCG@5_std': metrics.get('NDCG@5_std', 0.0),
        'NDCG@10_mean': metrics.get('NDCG@10', 0.0),
        'NDCG@10_std': metrics.get('NDCG@10_std', 0.0),
        'MAP_mean': metrics.get('MAP', 0.0),
        'MAP_std': metrics.get('MAP_std', 0.0),
        'Precision@1_mean': metrics.get('Precision@1', 0.0),
        'Precision@1_std': metrics.get('Precision@1_std', 0.0),
        'Precision@5_mean': metrics.get('Precision@5', 0.0),
        'Precision@5_std': metrics.get('Precision@5_std', 0.0),
        'Response_Time_mean': metrics.get('ART', 0.0),
        'Response_Time_std': metrics.get('ART_std', 0.0),
        'Success_Rate': metrics.get('Success_Rate', 0.0),
        'Kendall_Tau': metrics.get('Kendall_Tau', 0.0),
        'Spearman_Rho': metrics.get('Spearman_Rho', 0.0),
        # CRITICAL: Include individual metrics for statistical significance testing
        'individual_metrics': individual_metrics
    }
    
    return stats

def generate_report(statistics: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Generate a comprehensive evaluation report
    
    Args:
        statistics: List of model statistics
        
    Returns:
        Report dictionary
    """
    # Find statistics for each model
    try:
        mama_full = next(s for s in statistics if s['model'] == 'MAMA Full')
        mama_no_trust = next(s for s in statistics if s['model'] == 'MAMA No Trust')
        single_agent = next(s for s in statistics if s['model'] == 'Single Agent System')
        traditional = next(s for s in statistics if s['model'] == 'Traditional Ranking')
        
        # Calculate improvements
        trust_improvement = ((mama_full['MRR_mean'] - mama_no_trust['MRR_mean']) / 
                            mama_no_trust['MRR_mean'] * 100)
        multi_agent_improvement = ((mama_full['MRR_mean'] - single_agent['MRR_mean']) / 
                                single_agent['MRR_mean'] * 100)
        overall_improvement = ((mama_full['MRR_mean'] - traditional['MRR_mean']) / 
                            traditional['MRR_mean'] * 100)
        
        report = {
            'key_findings': [
                f"MAMA Full achieved best performance: MRR={mama_full['MRR_mean']:.4f}¬±{mama_full['MRR_std']:.4f}",
                f"Trust mechanism contributed {trust_improvement:.1f}% improvement",
                f"Multi-agent approach shows {multi_agent_improvement:.1f}% advantage",
                f"Overall improvement of {overall_improvement:.1f}% over traditional methods"
            ],
            'performance_ranking': [
                f"1. MAMA Full: {mama_full['MRR_mean']:.4f}",
                f"2. MAMA No Trust: {mama_no_trust['MRR_mean']:.4f}",
                f"3. Single Agent: {single_agent['MRR_mean']:.4f}",
                f"4. Traditional: {traditional['MRR_mean']:.4f}"
            ],
            'improvements': {
                'trust_improvement': float(trust_improvement),
                'multi_agent_improvement': float(multi_agent_improvement),
                'overall_improvement': float(overall_improvement)
            }
        }
        
        return report
    except (StopIteration, KeyError) as e:
        logger.error(f"Error generating report: {e}")
        return {
            'key_findings': ["Error generating report - missing model statistics"],
            'performance_ranking': [],
            'improvements': {}
        }

def run_evaluation(args):
    """
    Run comprehensive evaluation of all models
    
    Args:
        args: Command line arguments
        
    Returns:
        Path to output file
    """
    start_time = time.time()
    
    logger.info("üöÄ Starting comprehensive model evaluation - Full Academic Standard")
    logger.info("üéØ Using complete 150 test queries for rigorous evaluation")
    
    # Step 1: Setup model cache for performance optimization
    setup_model_cache()
    
    # Load test dataset
    try:
        test_queries = load_test_dataset()
        
        # üîß CRITICAL FIX: Always use full 150 test queries for academic integrity
        logger.info(f"üìä Using FULL test set: {len(test_queries)} queries (academic integrity)")
        logger.info("‚úÖ Complete evaluation as per paper methodology - no sampling shortcuts!")
        
    except FileNotFoundError:
        logger.error("Test dataset not found. Generating standard dataset...")
        # Try to import and run dataset generator
        try:
            from generate_datasets import generate_standard_dataset
            generate_standard_dataset()
            test_queries = load_test_dataset()
        except ImportError:
            logger.error("Could not import dataset generator")
            return None
    
    # Initialize evaluator
    evaluator = StandardEvaluator(random_seed=42)
    
    # Use cached models for better performance
    model_configs = [
        ('MAMA_Full', 'MAMA Full'),
        ('MAMA_NoTrust', 'MAMA No Trust'), 
        ('SingleAgent', 'Single Agent'),
        ('Traditional', 'Traditional')
    ]
    
    # üéØ ACADEMIC INTEGRITY: Use full test set - no shortcuts
    logger.info(f"üìä Evaluating all models on FULL test set: {len(test_queries)} queries")
    logger.info("‚úÖ Complete 4-model evaluation following paper methodology exactly!")
    
    # Run evaluation for each model - Complete 4-model comparison
    all_results = []
    
    # Sequential evaluation to ensure all models are tested
    for model_key, model_display_name in model_configs:
        logger.info(f"üìä Evaluating model: {model_display_name}")
        
        # Get cached model
        model = get_cached_model(model_key)
        if model is None:
            logger.error(f"‚ùå Could not load cached model: {model_key}")
            continue
        
        # Evaluate model using standard method
        result = evaluator.evaluate_model(
            model=model,
            test_data=test_queries,
            model_name=model_display_name
        )
        
        # Calculate statistics
        stats = calculate_statistics(result, model_display_name)
        all_results.append(stats)
        
        # Log completion
        logger.info(f"‚úÖ {model_display_name} evaluation completed")
    
    # Generate comprehensive report
    report = generate_report(all_results)
    
    # Perform statistical significance tests
    significance_tests = evaluator.perform_statistical_significance_tests(all_results)
    report['significance_tests'] = significance_tests
    
    # Generate significance table
    significance_table = evaluator.generate_significance_table(significance_tests)
    
    # Save significance table
    figures_dir = Path("figures")
    figures_dir.mkdir(exist_ok=True)
    
    with open(figures_dir / "table_1_statistical_significance.md", "w") as f:
        f.write(significance_table)
    
    # Save results to file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Save timestamped results
    timestamped_file = f"results/real_evaluation_{timestamp}.json"
    
    # Save final standardized results
    final_results_file = "results/final_results.json"
    
    # Ensure directory exists
    Path("results").mkdir(exist_ok=True)
    
    # Create comprehensive results structure for plotting
    final_results = {
        'evaluation_timestamp': timestamp,
        'evaluation_date': datetime.now().isoformat(),
        'total_queries': len(test_queries),
        'models_evaluated': len(all_results),
        'performance_statistics': all_results,
        'detailed_report': report,
        'significance_tests': significance_tests,
        'metadata': {
            'dataset_size': len(test_queries),
            'evaluation_duration_seconds': time.time() - start_time,
            'random_seed': 42,
            'evaluator_config': 'StandardEvaluator',
            'optimization_enabled': True,
            'model_caching': True,
            'batch_processing': True
        }
    }
    
    # Save both files
    with open(timestamped_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2)
    
    with open(final_results_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, indent=2)
    
    # Print summary
    print("\n" + "="*80)
    print("üìä MAMA Framework - Real Performance Results Summary")
    print("="*80)
    print(f"{'Model':<20} {'MRR':<12} {'NDCG@5':<12} {'ART (seconds)':<15}")
    print("-"*80)
    
    for result in all_results:
        model = result['model']
        mrr = f"{result['MRR_mean']:.4f}"
        ndcg = f"{result['NDCG@5_mean']:.4f}"
        art = f"{result['Response_Time_mean']:.2f}"
        print(f"{model:<20} {mrr:<12} {ndcg:<12} {art:<15}")
    
    print("-"*80)
    print(f"‚úÖ Evaluation completed in {time.time() - start_time:.2f} seconds")
    print(f"üìä Results saved to {final_results_file}")
    print("="*80)
    
    return final_results_file

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='MAMA Framework Main Evaluation')
    parser.add_argument('--data-path', type=str, default=None,
                        help='Path to test dataset JSON file')
    parser.add_argument('--no-cache', action='store_true',
                        help='Force rerunning all evaluations without using cache')
    parser.add_argument('--verbose', action='store_true',
                        help='Display detailed progress information')

    args = parser.parse_args()
    
    try:
        output_file = run_evaluation(args)
        return 0
    except Exception as e:
        logger.error(f"Evaluation failed: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main()) 
