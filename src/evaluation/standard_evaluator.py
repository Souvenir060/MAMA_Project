#!/usr/bin/env python3
"""
æ ‡å‡†åŒ–è¯„ä¼°å™¨ - MAMA ç³»ç»Ÿå­¦æœ¯å®éªŒ
ç¡®ä¿æ‰€æœ‰æ¨¡å‹ä½¿ç”¨ç›¸åŒçš„è¯„ä¼°æ ‡å‡†å’ŒæŒ‡æ ‡ï¼Œé¿å…è¯„ä¼°åå·®
"""

import json
import numpy as np
import time
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from sklearn.metrics import ndcg_score
from scipy.stats import kendalltau, spearmanr
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StandardEvaluator:
    """æ ‡å‡†åŒ–è¯„ä¼°å™¨"""
    
    def __init__(self, random_seed: int = 42):
        """
        åˆå§‹åŒ–æ ‡å‡†åŒ–è¯„ä¼°å™¨
        
        Args:
            random_seed: éšæœºç§å­ç¡®ä¿å¯å¤ç°æ€§
        """
        np.random.seed(random_seed)
        self.random_seed = random_seed
        
        # è¯„ä¼°æŒ‡æ ‡çš„å­¦æœ¯å®šä¹‰
        self.metrics_definitions = {
            'MRR': 'Mean Reciprocal Rank - å¹³å‡å€’æ•°æ’å',
            'NDCG@5': 'Normalized Discounted Cumulative Gain at 5',
            'NDCG@10': 'Normalized Discounted Cumulative Gain at 10',
            'MAP': 'Mean Average Precision - å¹³å‡ç²¾ç¡®åº¦',
            'ART': 'Average Response Time - å¹³å‡å“åº”æ—¶é—´',
            'Precision@1': 'Precision at 1 - ç¬¬ä¸€ä½ç²¾ç¡®åº¦',
            'Precision@5': 'Precision at 5 - å‰äº”ä½ç²¾ç¡®åº¦',
            'Kendall_Tau': 'Kendall Tau correlation coefficient',
            'Spearman_Rho': 'Spearman rank correlation coefficient'
        }
        
        # è®°å½•æ‰€æœ‰è¯„ä¼°ç»“æœ
        self.evaluation_history = []
        
    def load_test_data(self, test_file: str) -> List[Dict[str, Any]]:
        """
        åŠ è½½æµ‹è¯•æ•°æ®
        
        Args:
            test_file: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
            
        Returns:
            æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨
        """
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            logger.info(f"âœ… åŠ è½½æµ‹è¯•æ•°æ®: {len(test_data)} æ¡æŸ¥è¯¢")
            return test_data
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æµ‹è¯•æ•°æ®å¤±è´¥: {e}")
            return []
    
    def evaluate_model(self, model: Any, test_data: List[Dict[str, Any]], 
                      model_name: str = "Unknown") -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ¨¡å‹çš„æ€§èƒ½
        
        Args:
            model: å¾…è¯„ä¼°çš„æ¨¡å‹å®ä¾‹
            test_data: æµ‹è¯•æ•°æ®
            model_name: æ¨¡å‹åç§°
            
        Returns:
            å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        logger.info(f"ğŸ”„ å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        evaluation_start_time = time.time()
        
        # åˆå§‹åŒ–ç»“æœå­˜å‚¨
        results = {
            'model_name': model_name,
            'evaluation_start_time': datetime.now().isoformat(),
            'total_queries': len(test_data),
            'successful_queries': 0,
            'failed_queries': 0,
            'query_results': [],
            'response_times': [],
            'rankings': [],
            'relevance_scores': [],
            'ground_truth_rankings': []
        }
        
        # é€ä¸ªå¤„ç†æµ‹è¯•æŸ¥è¯¢
        for i, query_data in enumerate(test_data):
            try:
                # è®°å½•å•ä¸ªæŸ¥è¯¢çš„å¼€å§‹æ—¶é—´
                query_start_time = time.time()
                
                # è°ƒç”¨æ¨¡å‹å¤„ç†æŸ¥è¯¢
                model_result = self._call_model_safely(model, query_data)
                
                # è®°å½•å“åº”æ—¶é—´
                response_time = time.time() - query_start_time
                
                if model_result:
                    # å¤„ç†æ¨¡å‹è¾“å‡º
                    predicted_ranking = self._extract_ranking_from_result(model_result)
                    ground_truth_ranking = query_data['ground_truth_ranking']
                    relevance_scores = query_data['relevance_scores']
                    
                    # å­˜å‚¨ç»“æœ
                    results['query_results'].append({
                        'query_id': query_data['query_id'],
                        'predicted_ranking': predicted_ranking,
                        'ground_truth_ranking': ground_truth_ranking,
                        'relevance_scores': relevance_scores,
                        'response_time': response_time,
                        'model_result': model_result
                    })
                    
                    results['response_times'].append(response_time)
                    results['rankings'].append(predicted_ranking)
                    results['relevance_scores'].append(relevance_scores)
                    results['ground_truth_rankings'].append(ground_truth_ranking)
                    results['successful_queries'] += 1
                    
                else:
                    results['failed_queries'] += 1
                    logger.warning(f"âš ï¸ æŸ¥è¯¢ {query_data['query_id']} å¤„ç†å¤±è´¥")
                
                # è¿›åº¦æŠ¥å‘Š
                if (i + 1) % 50 == 0:
                    logger.info(f"ğŸ“Š å·²å¤„ç† {i + 1}/{len(test_data)} æ¡æŸ¥è¯¢")
                    
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æŸ¥è¯¢ {query_data.get('query_id', 'unknown')} æ—¶å‡ºé”™: {e}")
                results['failed_queries'] += 1
        
        # è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        if results['successful_queries'] > 0:
            metrics = self._calculate_comprehensive_metrics(results)
            results['metrics'] = metrics
        else:
            results['metrics'] = self._get_zero_metrics()
        
        # è®°å½•æ€»è¯„ä¼°æ—¶é—´
        results['total_evaluation_time'] = time.time() - evaluation_start_time
        results['evaluation_end_time'] = datetime.now().isoformat()
        
        # ä¿å­˜è¯„ä¼°å†å²
        self.evaluation_history.append(results)
        
        logger.info(f"âœ… æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ")
        logger.info(f"ğŸ“Š æˆåŠŸæŸ¥è¯¢: {results['successful_queries']}/{results['total_queries']}")
        logger.info(f"â±ï¸  æ€»ç”¨æ—¶: {results['total_evaluation_time']:.2f} ç§’")
        
        return results
    
    def _call_model_safely(self, model: Any, query_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """å®‰å…¨è°ƒç”¨æ¨¡å‹ï¼Œå¤„ç†å„ç§å¼‚å¸¸æƒ…å†µ"""
        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©è°ƒç”¨æ–¹å¼
            if hasattr(model, 'process_query'):
                return model.process_query(query_data)
            elif hasattr(model, 'predict'):
                return model.predict(query_data)
            elif hasattr(model, 'recommend'):
                return model.recommend(query_data)
            elif callable(model):
                return model(query_data)
            else:
                logger.error(f"âŒ æ¨¡å‹ç±»å‹ä¸æ”¯æŒ: {type(model)}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _extract_ranking_from_result(self, model_result: Dict[str, Any]) -> List[str]:
        """ä»æ¨¡å‹ç»“æœä¸­æå–æ’å"""
        if 'ranking' in model_result:
            return model_result['ranking']
        elif 'recommendations' in model_result:
            # ä»æ¨èç»“æœä¸­æå–æ’å
            recommendations = model_result['recommendations']
            if isinstance(recommendations, list):
                return [rec.get('flight_id', f"flight_{i:03d}") for i, rec in enumerate(recommendations)]
        elif 'predicted_ranking' in model_result:
            return model_result['predicted_ranking']
        else:
            # é»˜è®¤æ’å
            return [f"flight_{i:03d}" for i in range(1, 11)]
    
    def _calculate_comprehensive_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—ç»¼åˆè¯„ä¼°æŒ‡æ ‡"""
        metrics = {}
        
        # 1. Mean Reciprocal Rank (MRR)
        metrics['MRR'] = self._calculate_mrr(
            results['rankings'], 
            results['ground_truth_rankings']
        )
        
        # 2. NDCG@5 å’Œ NDCG@10
        metrics['NDCG@5'] = self._calculate_ndcg(
            results['rankings'], 
            results['relevance_scores'], 
            k=5
        )
        
        metrics['NDCG@10'] = self._calculate_ndcg(
            results['rankings'], 
            results['relevance_scores'], 
            k=10
        )
        
        # 3. Mean Average Precision (MAP)
        metrics['MAP'] = self._calculate_map(
            results['rankings'], 
            results['relevance_scores']
        )
        
        # 4. Average Response Time (ART)
        metrics['ART'] = np.mean(results['response_times'])
        
        # 5. Precision@1 å’Œ Precision@5
        metrics['Precision@1'] = self._calculate_precision_at_k(
            results['rankings'], 
            results['ground_truth_rankings'], 
            k=1
        )
        
        metrics['Precision@5'] = self._calculate_precision_at_k(
            results['rankings'], 
            results['ground_truth_rankings'], 
            k=5
        )
        
        # 6. Rank Correlation
        metrics['Kendall_Tau'] = self._calculate_kendall_tau(
            results['rankings'], 
            results['ground_truth_rankings']
        )
        
        metrics['Spearman_Rho'] = self._calculate_spearman_rho(
            results['rankings'], 
            results['ground_truth_rankings']
        )
        
        # 7. ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        metrics['Success_Rate'] = results['successful_queries'] / results['total_queries']
        metrics['Average_Response_Time'] = np.mean(results['response_times'])
        metrics['Response_Time_Std'] = np.std(results['response_times'])
        
        return metrics
    
    def _calculate_mrr(self, predicted_rankings: List[List[str]], 
                      ground_truth_rankings: List[List[str]]) -> float:
        """
        è®¡ç®—å¹³å‡å€’æ•°æ’å (MRR)
        MRR = 1/|Q| Ã— Î£(1/rank_i)
        """
        reciprocal_ranks = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not gt_ranking:
                continue
                
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªç›¸å…³é¡¹ç›®çš„ä½ç½®
            relevant_item = gt_ranking[0]  # æœ€ç›¸å…³çš„é¡¹ç›®
            try:
                rank = pred_ranking.index(relevant_item) + 1  # 1-indexed
                reciprocal_ranks.append(1.0 / rank)
            except ValueError:
                reciprocal_ranks.append(0.0)
        
        return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    def _calculate_ndcg(self, predicted_rankings: List[List[str]], 
                       relevance_scores_list: List[Dict[str, float]], 
                       k: int = 5) -> float:
        """
        è®¡ç®—å½’ä¸€åŒ–æŠ˜æ‰£ç´¯ç§¯å¢ç›Š (NDCG@k)
        """
        ndcg_scores = []
        
        for pred_ranking, relevance_scores in zip(predicted_rankings, relevance_scores_list):
            if not pred_ranking or not relevance_scores:
                continue
            
            # æ„å»ºçœŸå®ç›¸å…³æ€§å’Œé¢„æµ‹ç›¸å…³æ€§
            y_true = []
            y_score = []
            
            for item in pred_ranking[:k]:
                relevance = relevance_scores.get(item, 0.0)
                y_true.append(relevance)
                y_score.append(1.0)  # ç®€åŒ–çš„é¢„æµ‹åˆ†æ•°
            
            if len(y_true) > 0:
                try:
                    # ä½¿ç”¨sklearnçš„ndcg_score
                    ndcg = ndcg_score([y_true], [y_score], k=k)
                    ndcg_scores.append(ndcg)
                except:
                    # æ‰‹åŠ¨è®¡ç®—NDCG
                    dcg = self._calculate_dcg(y_true, k)
                    ideal_relevance = sorted(y_true, reverse=True)
                    idcg = self._calculate_dcg(ideal_relevance, k)
                    ndcg_scores.append(dcg / idcg if idcg > 0 else 0.0)
        
        return np.mean(ndcg_scores) if ndcg_scores else 0.0
    
    def _calculate_dcg(self, relevance_scores: List[float], k: int) -> float:
        """è®¡ç®—æŠ˜æ‰£ç´¯ç§¯å¢ç›Š (DCG)"""
        dcg = 0.0
        for i, rel in enumerate(relevance_scores[:k]):
            dcg += rel / np.log2(i + 2)  # i+2 because log2(1) = 0
        return dcg
    
    def _calculate_map(self, predicted_rankings: List[List[str]], 
                      relevance_scores_list: List[Dict[str, float]]) -> float:
        """è®¡ç®—å¹³å‡ç²¾ç¡®åº¦ (MAP)"""
        ap_scores = []
        
        for pred_ranking, relevance_scores in zip(predicted_rankings, relevance_scores_list):
            if not pred_ranking or not relevance_scores:
                continue
            
            # è®¡ç®—Average Precision
            relevant_items = []
            precision_at_k = []
            
            for i, item in enumerate(pred_ranking):
                if relevance_scores.get(item, 0.0) > 0.5:  # ç›¸å…³é˜ˆå€¼
                    relevant_items.append(i + 1)
                    precision_at_k.append(len(relevant_items) / (i + 1))
            
            if relevant_items:
                ap = np.mean(precision_at_k)
                ap_scores.append(ap)
        
        return np.mean(ap_scores) if ap_scores else 0.0
    
    def _calculate_precision_at_k(self, predicted_rankings: List[List[str]], 
                                 ground_truth_rankings: List[List[str]], 
                                 k: int) -> float:
        """è®¡ç®—P@kç²¾ç¡®åº¦"""
        precisions = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not pred_ranking or not gt_ranking:
                continue
            
            # è®¡ç®—å‰kä¸ªé¢„æµ‹ä¸­æœ‰å¤šå°‘æ˜¯ç›¸å…³çš„
            relevant_set = set(gt_ranking[:k])
            predicted_set = set(pred_ranking[:k])
            
            intersection = relevant_set.intersection(predicted_set)
            precision = len(intersection) / k if k > 0 else 0.0
            precisions.append(precision)
        
        return np.mean(precisions) if precisions else 0.0
    
    def _calculate_kendall_tau(self, predicted_rankings: List[List[str]], 
                              ground_truth_rankings: List[List[str]]) -> float:
        """è®¡ç®—Kendall Tauç›¸å…³ç³»æ•°"""
        correlations = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not pred_ranking or not gt_ranking:
                continue
            
            # åˆ›å»ºæ’åæ˜ å°„
            pred_ranks = {item: i for i, item in enumerate(pred_ranking)}
            gt_ranks = {item: i for i, item in enumerate(gt_ranking)}
            
            # æ‰¾åˆ°å…±åŒé¡¹ç›®
            common_items = set(pred_ranks.keys()).intersection(set(gt_ranks.keys()))
            
            if len(common_items) > 1:
                pred_vals = [pred_ranks[item] for item in common_items]
                gt_vals = [gt_ranks[item] for item in common_items]
                
                try:
                    tau, _ = kendalltau(pred_vals, gt_vals)
                    if not np.isnan(tau):
                        correlations.append(tau)
                except:
                    pass
        
        return np.mean(correlations) if correlations else 0.0
    
    def _calculate_spearman_rho(self, predicted_rankings: List[List[str]], 
                               ground_truth_rankings: List[List[str]]) -> float:
        """è®¡ç®—Spearmanç›¸å…³ç³»æ•°"""
        correlations = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not pred_ranking or not gt_ranking:
                continue
            
            # åˆ›å»ºæ’åæ˜ å°„
            pred_ranks = {item: i for i, item in enumerate(pred_ranking)}
            gt_ranks = {item: i for i, item in enumerate(gt_ranking)}
            
            # æ‰¾åˆ°å…±åŒé¡¹ç›®
            common_items = set(pred_ranks.keys()).intersection(set(gt_ranks.keys()))
            
            if len(common_items) > 1:
                pred_vals = [pred_ranks[item] for item in common_items]
                gt_vals = [gt_ranks[item] for item in common_items]
                
                try:
                    rho, _ = spearmanr(pred_vals, gt_vals)
                    if not np.isnan(rho):
                        correlations.append(rho)
                except:
                    pass
        
        return np.mean(correlations) if correlations else 0.0
    
    def _get_zero_metrics(self) -> Dict[str, Any]:
        """è¿”å›é›¶å€¼æŒ‡æ ‡ï¼ˆå½“è¯„ä¼°å¤±è´¥æ—¶ï¼‰"""
        return {
            'MRR': 0.0,
            'NDCG@5': 0.0,
            'NDCG@10': 0.0,
            'MAP': 0.0,
            'ART': 0.0,
            'Precision@1': 0.0,
            'Precision@5': 0.0,
            'Kendall_Tau': 0.0,
            'Spearman_Rho': 0.0,
            'Success_Rate': 0.0,
            'Average_Response_Time': 0.0,
            'Response_Time_Std': 0.0
        }
    
    def generate_comparison_report(self, results_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆå¤šä¸ªæ¨¡å‹çš„å¯¹æ¯”æŠ¥å‘Š"""
        if not results_list:
            return {}
        
        # æ±‡æ€»æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡
        comparison_data = {}
        
        for result in results_list:
            model_name = result['model_name']
            metrics = result['metrics']
            
            comparison_data[model_name] = {
                'MRR': metrics['MRR'],
                'NDCG@5': metrics['NDCG@5'],
                'NDCG@10': metrics['NDCG@10'],
                'MAP': metrics['MAP'],
                'ART': metrics['ART'],
                'Precision@1': metrics['Precision@1'],
                'Precision@5': metrics['Precision@5'],
                'Success_Rate': metrics['Success_Rate'],
                'Kendall_Tau': metrics['Kendall_Tau'],
                'Spearman_Rho': metrics['Spearman_Rho']
            }
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_models = {}
        for metric in ['MRR', 'NDCG@5', 'MAP', 'Precision@1']:
            best_model = max(comparison_data.keys(), 
                           key=lambda x: comparison_data[x][metric])
            best_models[metric] = best_model
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'comparison_data': comparison_data,
            'best_models': best_models,
            'total_models': len(results_list),
            'evaluation_date': datetime.now().isoformat(),
            'metrics_definitions': self.metrics_definitions
        }
        
        return report
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def evaluate_single_agent_output(self, agent_output: Dict[str, Any], 
                                     ground_truth: Dict[str, Any], 
                                     agent_type: str) -> float:
        """
        è¯„ä¼°å•ä¸ªæ™ºèƒ½ä½“è¾“å‡ºçš„çœŸå®å‡†ç¡®æ€§åˆ†æ•°
        
        Args:
            agent_output: æ™ºèƒ½ä½“çš„è¾“å‡º
            ground_truth: å¯¹åº”çš„ground truthæ•°æ®
            agent_type: æ™ºèƒ½ä½“ç±»å‹ï¼ˆç”¨äºé€‰æ‹©è¯„ä¼°ç­–ç•¥ï¼‰
        
        Returns:
            çœŸå®çš„å‡†ç¡®æ€§åˆ†æ•° (0.0 - 1.0)
        """
        try:
            # æ ¹æ®æ™ºèƒ½ä½“ç±»å‹é‡‡ç”¨ä¸åŒçš„è¯„ä¼°ç­–ç•¥
            if 'safety' in agent_type.lower():
                return self._evaluate_safety_agent(agent_output, ground_truth)
            elif 'economic' in agent_type.lower():
                return self._evaluate_economic_agent(agent_output, ground_truth)
            elif 'weather' in agent_type.lower():
                return self._evaluate_weather_agent(agent_output, ground_truth)
            elif 'flight' in agent_type.lower():
                return self._evaluate_flight_agent(agent_output, ground_truth)
            else:
                # é€šç”¨è¯„ä¼°æ–¹æ³•
                return self._evaluate_generic_agent(agent_output, ground_truth)
                
        except Exception as e:
            logger.error(f"å•æ™ºèƒ½ä½“è¯„ä¼°å¤±è´¥ {agent_type}: {e}")
            return 0.0
    
    def _evaluate_safety_agent(self, agent_output: Dict[str, Any], 
                              ground_truth: Dict[str, Any]) -> float:
        """è¯„ä¼°å®‰å…¨è¯„ä¼°æ™ºèƒ½ä½“"""
        try:
            result = agent_output.get('result', {})
            gt_safety = ground_truth.get('safety_score', 0.0)
            
            if isinstance(result, dict):
                # æå–å®‰å…¨åˆ†æ•°
                predicted_safety = result.get('overall_safety_score', 
                                            result.get('safety_score', 
                                                     result.get('score', 0.5)))
            else:
                predicted_safety = 0.5
            
            # è®¡ç®—å‡†ç¡®æ€§ï¼šåŸºäºä¸ground truthçš„æ¥è¿‘ç¨‹åº¦
            error = abs(predicted_safety - gt_safety)
            accuracy = max(0.0, 1.0 - error)
            
            return accuracy
            
        except Exception as e:
            logger.warning(f"å®‰å…¨æ™ºèƒ½ä½“è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_economic_agent(self, agent_output: Dict[str, Any], 
                                ground_truth: Dict[str, Any]) -> float:
        """è¯„ä¼°ç»æµæ™ºèƒ½ä½“"""
        try:
            result = agent_output.get('result', {})
            gt_cost = ground_truth.get('economic_score', 0.0)
            
            if isinstance(result, dict):
                # æå–ç»æµåˆ†æ•°
                predicted_cost = result.get('total_cost_per_flight', 
                                          result.get('cost_score', 
                                                   result.get('economic_score', 
                                                            result.get('score', 0.5))))
            else:
                predicted_cost = 0.5
            
            # æ ‡å‡†åŒ–å¤„ç†
            if gt_cost > 0:
                error = abs(predicted_cost - gt_cost) / max(gt_cost, predicted_cost)
                accuracy = max(0.0, 1.0 - error)
            else:
                accuracy = 0.5
            
            return accuracy
            
        except Exception as e:
            logger.warning(f"ç»æµæ™ºèƒ½ä½“è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_weather_agent(self, agent_output: Dict[str, Any], 
                               ground_truth: Dict[str, Any]) -> float:
        """è¯„ä¼°å¤©æ°”æ™ºèƒ½ä½“"""
        try:
            result = agent_output.get('result', {})
            gt_weather = ground_truth.get('weather_score', 0.0)
            
            if isinstance(result, dict):
                predicted_weather = result.get('safety_score', 
                                             result.get('weather_score', 
                                                      result.get('score', 0.5)))
            else:
                predicted_weather = 0.5
            
            error = abs(predicted_weather - gt_weather)
            accuracy = max(0.0, 1.0 - error)
            
            return accuracy
            
        except Exception as e:
            logger.warning(f"å¤©æ°”æ™ºèƒ½ä½“è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_flight_agent(self, agent_output: Dict[str, Any], 
                              ground_truth: Dict[str, Any]) -> float:
        """è¯„ä¼°èˆªç­ä¿¡æ¯æ™ºèƒ½ä½“"""
        try:
            result = agent_output.get('result', {})
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–èˆªç­ä¿¡æ¯
            if isinstance(result, dict) and 'flight_list' in result:
                flight_list = result['flight_list']
                if flight_list and len(flight_list) > 0:
                    # åŸºäºè·å–åˆ°çš„èˆªç­æ•°é‡è¯„ä¼°
                    expected_count = ground_truth.get('expected_flight_count', 5)
                    actual_count = len(flight_list)
                    
                    # è®¡ç®—è¦†ç›–ç‡
                    coverage = min(1.0, actual_count / expected_count)
                    return coverage
                else:
                    return 0.0
            else:
                return 0.5
                
        except Exception as e:
            logger.warning(f"èˆªç­æ™ºèƒ½ä½“è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _evaluate_generic_agent(self, agent_output: Dict[str, Any], 
                               ground_truth: Dict[str, Any]) -> float:
        """é€šç”¨æ™ºèƒ½ä½“è¯„ä¼°"""
        try:
            # åŸºäºè¾“å‡ºçš„å®Œæ•´æ€§å’Œè´¨é‡è¯„ä¼°
            result = agent_output.get('result', {})
            success = agent_output.get('success', True)
            confidence = agent_output.get('confidence', 0.5)
            
            if not success:
                return 0.0
            
            # å¦‚æœæœ‰ç‰¹å®šçš„åˆ†æ•°å­—æ®µ
            if isinstance(result, dict):
                score = result.get('score', result.get('confidence', confidence))
                return min(1.0, max(0.0, score))
            else:
                return confidence
                
        except Exception as e:
            logger.warning(f"é€šç”¨æ™ºèƒ½ä½“è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def print_metrics_summary(self, results: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æŒ‡æ ‡æ‘˜è¦"""
        model_name = results['model_name']
        metrics = results['metrics']
        
        print(f"\nğŸ“Š æ¨¡å‹ {model_name} è¯„ä¼°ç»“æœ:")
        print("=" * 60)
        print(f"ğŸ“ˆ MRR (Mean Reciprocal Rank): {metrics['MRR']:.4f}")
        print(f"ğŸ“ˆ NDCG@5: {metrics['NDCG@5']:.4f}")
        print(f"ğŸ“ˆ NDCG@10: {metrics['NDCG@10']:.4f}")
        print(f"ğŸ“ˆ MAP (Mean Average Precision): {metrics['MAP']:.4f}")
        print(f"ğŸ“ˆ Precision@1: {metrics['Precision@1']:.4f}")
        print(f"ğŸ“ˆ Precision@5: {metrics['Precision@5']:.4f}")
        print(f"â±ï¸  ART (Average Response Time): {metrics['ART']:.4f}s")
        print(f"âœ… Success Rate: {metrics['Success_Rate']:.4f}")
        print(f"ğŸ”— Kendall Tau: {metrics['Kendall_Tau']:.4f}")
        print(f"ğŸ”— Spearman Rho: {metrics['Spearman_Rho']:.4f}")
        print("=" * 60) 