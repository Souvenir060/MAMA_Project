#!/usr/bin/env python3
"""
Standard Evaluator 
Ensures all models use the same evaluation standards and metrics
"""

import json
import numpy as np
import time
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from sklearn.metrics import ndcg_score
from scipy.stats import kendalltau, spearmanr
import logging
import pandas as pd
import scipy.stats as stats

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class StandardEvaluator:
    """Standard Evaluator"""
    
    def __init__(self, random_seed: int = 42):
        """
        Initialize standard evaluator
        
        Args:
            random_seed: Random seed for reproducibility
        """
        np.random.seed(random_seed)
        self.random_seed = random_seed
        
        # Definitions of evaluation metrics
        self.metrics_definitions = {
            'MRR': 'Mean Reciprocal Rank',
            'NDCG@5': 'Normalized Discounted Cumulative Gain at 5',
            'NDCG@10': 'Normalized Discounted Cumulative Gain at 10',
            'MAP': 'Mean Average Precision',
            'ART': 'Average Response Time',
            'Precision@1': 'Precision at 1',
            'Precision@5': 'Precision at 5',
            'Kendall_Tau': 'Kendall Tau correlation coefficient',
            'Spearman_Rho': 'Spearman rank correlation coefficient'
        }
        
        # Record all evaluation results
        self.evaluation_history = []
        
    def load_test_data(self, test_file: str) -> List[Dict[str, Any]]:
        """
        Load test data
        
        Args:
            test_file: Test data file path
            
        Returns:
            List of test queries
        """
        try:
            with open(test_file, 'r', encoding='utf-8') as f:
                test_data = json.load(f)
            
            logger.info(f"âœ… Loaded test data: {len(test_data)} queries")
            return test_data
            
        except Exception as e:
            logger.error(f"âŒ Failed to load test data: {e}")
            return []
    
    def evaluate_model(self, model: Any, test_data: List[Dict[str, Any]], 
                      model_name: str = "Unknown") -> Dict[str, Any]:
        """
        Evaluate single model performance
        
        Args:
            model: Model instance to evaluate
            test_data: Test data
            model_name: Model name
            
        Returns:
            Complete evaluation results with individual metrics for statistical analysis
        """
        logger.info(f"ðŸ”„ Starting evaluation for model: {model_name}")
        
        # Record start time with high precision
        evaluation_start_time = time.perf_counter()
        
        # Initialize result storage
        results = {
            'model_name': model_name,
            'evaluation_start_time': datetime.now().isoformat(),
            'total_queries': len(test_data),
            'successful_queries': 0,
            'failed_queries': 0,
            'query_results': [],
            'response_times': [],
            'rankings': [],
            'relevance_scores': [],
            'ground_truth_rankings': []
        }
        
        # Storage for individual metrics (required for statistical significance testing)
        individual_metrics = {
            'individual_mrr': [],
            'individual_ndcg5': [],
            'individual_ndcg10': [],
            'individual_map': [],
            'individual_precision1': [],
            'individual_precision5': [],
            'individual_response_times': []
        }
        
        # Process test queries one by one
        for i, query_data in enumerate(test_data):
            try:
                # Record query start time with high precision
                query_start_time = time.perf_counter()
                
                # Call model to process query
                model_result = self._call_model_safely(model, query_data)
                
                # Record response time with high precision
                response_time = time.perf_counter() - query_start_time
                
                if model_result:
                    # Process model output
                    predicted_ranking = self._extract_ranking_from_result(model_result)
                    ground_truth_ranking = query_data.get('ground_truth_ranking', [])
                    
                    # Generate relevance_scores if not present
                    if 'relevance_scores' in query_data:
                        relevance_scores = query_data['relevance_scores']
                    else:
                        relevance_scores = self._generate_relevance_scores_from_flight_data(query_data)
                    
                    # Calculate individual metrics for this query
                    query_mrr = self._calculate_single_query_mrr(predicted_ranking, ground_truth_ranking)
                    query_ndcg5 = self._calculate_single_query_ndcg(predicted_ranking, relevance_scores, k=5)
                    query_ndcg10 = self._calculate_single_query_ndcg(predicted_ranking, relevance_scores, k=10)
                    query_map = self._calculate_single_query_ap(predicted_ranking, relevance_scores)
                    query_precision1 = self._calculate_single_query_precision_at_k(predicted_ranking, ground_truth_ranking, k=1)
                    query_precision5 = self._calculate_single_query_precision_at_k(predicted_ranking, ground_truth_ranking, k=5)
                    
                    # Store individual metrics
                    individual_metrics['individual_mrr'].append(query_mrr)
                    individual_metrics['individual_ndcg5'].append(query_ndcg5)
                    individual_metrics['individual_ndcg10'].append(query_ndcg10)
                    individual_metrics['individual_map'].append(query_map)
                    individual_metrics['individual_precision1'].append(query_precision1)
                    individual_metrics['individual_precision5'].append(query_precision5)
                    individual_metrics['individual_response_times'].append(response_time)
                    
                    # Store results
                    results['query_results'].append({
                        'query_id': query_data['query_id'],
                        'predicted_ranking': predicted_ranking,
                        'ground_truth_ranking': ground_truth_ranking,
                        'relevance_scores': relevance_scores,
                        'response_time': response_time,
                        'model_result': model_result,
                        'query_mrr': query_mrr,
                        'query_ndcg5': query_ndcg5
                    })
                    
                    results['response_times'].append(response_time)
                    results['rankings'].append(predicted_ranking)
                    results['relevance_scores'].append(relevance_scores)
                    results['ground_truth_rankings'].append(ground_truth_ranking)
                    results['successful_queries'] += 1
                
                else:
                    # Failed query
                    results['failed_queries'] += 1
                    # Add zero metrics for failed queries to maintain list alignment
                    individual_metrics['individual_mrr'].append(0.0)
                    individual_metrics['individual_ndcg5'].append(0.0)
                    individual_metrics['individual_ndcg10'].append(0.0)
                    individual_metrics['individual_map'].append(0.0)
                    individual_metrics['individual_precision1'].append(0.0)
                    individual_metrics['individual_precision5'].append(0.0)
                    individual_metrics['individual_response_times'].append(response_time)
                
            except Exception as e:
                logger.error(f"âŒ Error evaluating query {i}: {e}")
                results['failed_queries'] += 1
                # Add zero metrics for failed queries
                individual_metrics['individual_mrr'].append(0.0)
                individual_metrics['individual_ndcg5'].append(0.0)
                individual_metrics['individual_ndcg10'].append(0.0)
                individual_metrics['individual_map'].append(0.0)
                individual_metrics['individual_precision1'].append(0.0)
                individual_metrics['individual_precision5'].append(0.0)
                individual_metrics['individual_response_times'].append(0.0)
        
        # Calculate overall metrics
        metrics = self._calculate_comprehensive_metrics(results)
        
        # Add standard deviations to metrics
        if individual_metrics['individual_mrr']:
            metrics['MRR_std'] = np.std(individual_metrics['individual_mrr'])
            metrics['NDCG@5_std'] = np.std(individual_metrics['individual_ndcg5'])
            metrics['NDCG@10_std'] = np.std(individual_metrics['individual_ndcg10'])
            metrics['MAP_std'] = np.std(individual_metrics['individual_map'])
            metrics['Precision@1_std'] = np.std(individual_metrics['individual_precision1'])
            metrics['Precision@5_std'] = np.std(individual_metrics['individual_precision5'])
            metrics['ART_std'] = np.std(individual_metrics['individual_response_times'])
        else:
            metrics.update({
                'MRR_std': 0.0, 'NDCG@5_std': 0.0, 'NDCG@10_std': 0.0,
                'MAP_std': 0.0, 'Precision@1_std': 0.0, 'Precision@5_std': 0.0,
                'ART_std': 0.0
            })
        
        # Calculate evaluation duration
        evaluation_duration = time.perf_counter() - evaluation_start_time
        
        # Compile final evaluation result
        evaluation_result = {
            'model_name': model_name,
            'evaluation_duration': evaluation_duration,
            'metrics': metrics,
            'individual_metrics': individual_metrics,  # CRITICAL: Include individual metrics for significance testing
            'query_results': results['query_results'],
            'summary': {
                'total_queries': results['total_queries'],
                'successful_queries': results['successful_queries'],
                'failed_queries': results['failed_queries'],
                'success_rate': results['successful_queries'] / results['total_queries']
            }
        }
        
        logger.info(f"âœ… {model_name} evaluation completed: "
                   f"{results['successful_queries']}/{results['total_queries']} successful queries")
        logger.info(f"ðŸ“Š {model_name} MRR: {metrics['MRR']:.4f}Â±{metrics['MRR_std']:.4f}")
        
        return evaluation_result
    
    def _generate_relevance_scores_from_flight_data(self, query_data: Dict[str, Any]) -> Dict[str, float]:
        """
        Generate relevance scores from flight candidates data
        
        ðŸ“Š PAPER-COMPLIANT: Standard relevance scores from test data
        
        Args:
            query_data: Query data containing flight_candidates
            
        Returns:
            Dictionary mapping flight_id to relevance score
        """
        relevance_scores = {}
        
        # CRITICAL FIX: First try to use actual relevance scores from test data
        if 'relevance_scores' in query_data:
            # Use relevance scores from test data
            relevance_scores = query_data['relevance_scores'].copy()
            logger.debug(f"Using relevance scores: {len(relevance_scores)} items")
            return relevance_scores
        
        # Get flight candidates and ground truth ranking
        flight_candidates = query_data.get('flight_candidates', [])
        if not flight_candidates:
            flight_candidates = query_data.get('candidate_flights', [])
        
        ground_truth_ranking = query_data.get('ground_truth_ranking', [])
        
        if not flight_candidates:
            return relevance_scores
        
        # Create flight data mapping
        flight_data = {f.get('flight_id', f'flight_{i}'): f for i, f in enumerate(flight_candidates)}
        
                    # Standard multi-level relevance scores
        if ground_truth_ranking:
            # Use ranking position to assign relevance scores (higher rank = higher relevance)
            max_relevance = 1.0
            num_items = len(ground_truth_ranking)
            
            for rank, flight_id in enumerate(ground_truth_ranking):
                # Exponential decay: top items get much higher scores
                relevance = max_relevance * (0.9 ** rank)  # Stronger differentiation
                relevance_scores[flight_id] = relevance
                
        else:
            # Fallback: Generate relevance based on flight attributes with more differentiation
            relevance_standard = query_data.get('relevance_standard', 'balanced')
            
            for flight_id, flight in flight_data.items():
                # Extract scores
                safety_score = flight.get('safety_score', 0.5)
                price_score = flight.get('price_score', 0.5)  
                convenience_score = flight.get('convenience_score', 0.5)
                weather_score = flight.get('weather_score', 0.5)
                
                # Standard relevance calculation
                if relevance_standard == "safety_first":
                    # Safety-first: exponential weighting for safety
                    relevance = (safety_score ** 1.5) * 0.7 + price_score * 0.2 + convenience_score * 0.1
                elif relevance_standard == "budget":
                    # Budget: exponential weighting for price
                    relevance = (price_score ** 1.5) * 0.7 + safety_score * 0.2 + convenience_score * 0.1
                elif relevance_standard == "convenience":
                    # Convenience: exponential weighting for convenience
                    relevance = (convenience_score ** 1.5) * 0.7 + safety_score * 0.2 + price_score * 0.1
                else:  # balanced
                    # Balanced with slight non-linearity for differentiation
                    relevance = (safety_score * 0.35 + price_score * 0.35 + 
                               convenience_score * 0.2 + weather_score * 0.1)
                    # Add non-linear transformation for better differentiation
                    relevance = relevance ** 1.2
                
                # Add small random variation for tie-breaking (seeded for reproducibility)
                import random
                random.seed(42 + hash(flight_id))
                relevance += random.uniform(-0.05, 0.05)
                
                # Ensure valid range with better spread
                relevance_scores[flight_id] = min(1.0, max(0.1, relevance))
        
        return relevance_scores
    
    def _call_model_safely(self, model: Any, query_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """Safely call model, handle various exceptions"""
        try:
            # Choose calling method based on model type
            if hasattr(model, 'process_query'):
                return model.process_query(query_data)
            elif hasattr(model, 'predict'):
                return model.predict(query_data)
            elif hasattr(model, 'recommend'):
                return model.recommend(query_data)
            elif callable(model):
                return model(query_data)
            else:
                logger.error(f"âŒ Unsupported model type: {type(model)}")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Model call failed: {e}")
            return None
    
    def _extract_ranking_from_result(self, model_result: Dict[str, Any]) -> List[str]:
        """Extract ranking from model result"""
        if 'ranking' in model_result:
            return model_result['ranking']
        elif 'recommendations' in model_result:
            # Extract ranking from recommendations
            recommendations = model_result['recommendations']
            if isinstance(recommendations, list):
                return [rec.get('flight_id', f"flight_{i:03d}") for i, rec in enumerate(recommendations)]
        elif 'predicted_ranking' in model_result:
            return model_result['predicted_ranking']
        else:
            # Default ranking
            return [f"flight_{i:03d}" for i in range(1, 11)]
    
    def _calculate_comprehensive_metrics(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate comprehensive evaluation metrics"""
        metrics = {}
        
        # 1. Mean Reciprocal Rank (MRR)
        metrics['MRR'] = self._calculate_mrr(
            results['rankings'], 
            results['ground_truth_rankings']
        )
        
        # 2. NDCG@5 and NDCG@10
        metrics['NDCG@5'] = self._calculate_ndcg(
            results['rankings'], 
            results['relevance_scores'], 
            k=5
        )
        
        metrics['NDCG@10'] = self._calculate_ndcg(
            results['rankings'], 
            results['relevance_scores'], 
            k=10
        )
        
        # 3. Mean Average Precision (MAP)
        metrics['MAP'] = self._calculate_map(
            results['rankings'], 
            results['relevance_scores']
        )
        
        # 4. Average Response Time (ART)
        metrics['ART'] = np.mean(results['response_times'])
        
        # 5. Precision@1 and Precision@5
        metrics['Precision@1'] = self._calculate_precision_at_k(
            results['rankings'], 
            results['ground_truth_rankings'], 
            k=1
        )
        
        metrics['Precision@5'] = self._calculate_precision_at_k(
            results['rankings'], 
            results['ground_truth_rankings'], 
            k=5
        )
        
        # 6. Rank Correlation
        metrics['Kendall_Tau'] = self._calculate_kendall_tau(
            results['rankings'], 
            results['ground_truth_rankings']
        )
        
        metrics['Spearman_Rho'] = self._calculate_spearman_rho(
            results['rankings'], 
            results['ground_truth_rankings']
        )
        
        # 7. System performance metrics
        metrics['Success_Rate'] = results['successful_queries'] / results['total_queries']
        metrics['Average_Response_Time'] = np.mean(results['response_times'])
        metrics['Response_Time_Std'] = np.std(results['response_times'])
        
        return metrics
    
    def _calculate_mrr(self, predicted_rankings: List[List[str]], 
                      ground_truth_rankings: List[List[str]]) -> float:
        """
        Calculate Mean Reciprocal Rank (MRR)
        MRR = 1/|Q| Ã— Î£(1/rank_i)
        """
        reciprocal_ranks = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not gt_ranking:
                continue
                
            # Find position of first relevant item
            relevant_item = gt_ranking[0]  # Most relevant item
            try:
                rank = pred_ranking.index(relevant_item) + 1  # 1-indexed
                reciprocal_ranks.append(1.0 / rank)
            except ValueError:
                reciprocal_ranks.append(0.0)
        
        return np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    def _calculate_ndcg(self, predicted_rankings: List[List[str]], 
                       relevance_scores_list: List[Dict[str, float]], 
                       k: int = 5) -> float:
        """
        Calculate with multi-level relevance
        
        FIX: Use actual relevance scores instead of binary 0/1 to ensure differentiation
        """
        ndcg_scores = []
        
        for pred_ranking, relevance_scores in zip(predicted_rankings, relevance_scores_list):
            if not pred_ranking or not relevance_scores:
                continue
            
            # Use relevance scores
            predicted_relevance = []
            for item in pred_ranking[:k]:
                relevance = relevance_scores.get(item, 0.0)
                predicted_relevance.append(relevance)
            
            if len(predicted_relevance) > 0:
                try:
                    # Calculate DCG for predicted ranking
                    dcg = self._calculate_dcg(predicted_relevance, k)
                    
                    # Calculate IDEAL DCG using optimal ranking
                    all_relevances = list(relevance_scores.values())
                    ideal_relevance = sorted(all_relevances, reverse=True)[:k]
                    
                    # Pad with zeros if needed
                    while len(ideal_relevance) < k:
                        ideal_relevance.append(0.0)
                    
                    idcg = self._calculate_dcg(ideal_relevance, k)
                    
                    # Calculate NDCG with proper normalization
                    if idcg > 0:
                        ndcg = dcg / idcg
                    else:
                        ndcg = 0.0
                    
                    ndcg_scores.append(ndcg)
                    
                except Exception as e:
                    logger.warning(f"NDCG calculation failed: {e}")
                    ndcg_scores.append(0.0)
        
        return np.mean(ndcg_scores) if ndcg_scores else 0.0
    
    def _calculate_dcg(self, relevance_scores: List[float], k: int) -> float:
        """Calculate Discounted Cumulative Gain (DCG)"""
        dcg = 0.0
        for i, rel in enumerate(relevance_scores[:k]):
            dcg += rel / np.log2(i + 2)  # i+2 because log2(1) = 0
        return dcg
    
    def _calculate_map(self, predicted_rankings: List[List[str]], 
                      relevance_scores_list: List[Dict[str, float]]) -> float:
        """Calculate Mean Average Precision (MAP)"""
        ap_scores = []
        
        for pred_ranking, relevance_scores in zip(predicted_rankings, relevance_scores_list):
            if not pred_ranking or not relevance_scores:
                continue
            
            # Calculate Average Precision
            relevant_items = []
            precision_at_k = []
            
            for i, item in enumerate(pred_ranking):
                if relevance_scores.get(item, 0.0) > 0.5:  # Relevance threshold
                    relevant_items.append(i + 1)
                    precision_at_k.append(len(relevant_items) / (i + 1))
            
            if relevant_items:
                ap = np.mean(precision_at_k)
                ap_scores.append(ap)
        
        return np.mean(ap_scores) if ap_scores else 0.0
    
    def _calculate_precision_at_k(self, predicted_rankings: List[List[str]], 
                                 ground_truth_rankings: List[List[str]], 
                                 k: int) -> float:
        """Calculate P@k precision"""
        precisions = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not pred_ranking or not gt_ranking:
                continue
            
            # Calculate how many of top k predictions are relevant
            relevant_set = set(gt_ranking[:k])
            predicted_set = set(pred_ranking[:k])
            
            intersection = relevant_set.intersection(predicted_set)
            precision = len(intersection) / k if k > 0 else 0.0
            precisions.append(precision)
        
        return np.mean(precisions) if precisions else 0.0
    
    def _calculate_kendall_tau(self, predicted_rankings: List[List[str]], 
                              ground_truth_rankings: List[List[str]]) -> float:
        """Calculate Kendall Tau correlation coefficient"""
        correlations = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not pred_ranking or not gt_ranking:
                continue
            
            # Create ranking mappings
            pred_ranks = {item: i for i, item in enumerate(pred_ranking)}
            gt_ranks = {item: i for i, item in enumerate(gt_ranking)}
            
            # Find common items
            common_items = set(pred_ranks.keys()).intersection(set(gt_ranks.keys()))
            
            if len(common_items) > 1:
                pred_vals = [pred_ranks[item] for item in common_items]
                gt_vals = [gt_ranks[item] for item in common_items]
                
                try:
                    tau, _ = kendalltau(pred_vals, gt_vals)
                    if not np.isnan(tau):
                        correlations.append(tau)
                except:
                    pass
        
        return np.mean(correlations) if correlations else 0.0
    
    def _calculate_spearman_rho(self, predicted_rankings: List[List[str]], 
                               ground_truth_rankings: List[List[str]]) -> float:
        """Calculate Spearman correlation coefficient"""
        correlations = []
        
        for pred_ranking, gt_ranking in zip(predicted_rankings, ground_truth_rankings):
            if not pred_ranking or not gt_ranking:
                continue
            
            # Create ranking mappings
            pred_ranks = {item: i for i, item in enumerate(pred_ranking)}
            gt_ranks = {item: i for i, item in enumerate(gt_ranking)}
            
            # Find common items
            common_items = set(pred_ranks.keys()).intersection(set(gt_ranks.keys()))
            
            if len(common_items) > 1:
                pred_vals = [pred_ranks[item] for item in common_items]
                gt_vals = [gt_ranks[item] for item in common_items]
                
                try:
                    rho, _ = spearmanr(pred_vals, gt_vals)
                    if not np.isnan(rho):
                        correlations.append(rho)
                except:
                    pass
        
        return np.mean(correlations) if correlations else 0.0
    
    def _get_zero_metrics(self) -> Dict[str, Any]:
        """Return zero metrics (when evaluation fails)"""
        return {
            'MRR': 0.0,
            'NDCG@5': 0.0,
            'NDCG@10': 0.0,
            'MAP': 0.0,
            'ART': 0.0,
            'Precision@1': 0.0,
            'Precision@5': 0.0,
            'Kendall_Tau': 0.0,
            'Spearman_Rho': 0.0,
            'Success_Rate': 0.0,
            'Average_Response_Time': 0.0,
            'Response_Time_Std': 0.0
        }
    
    def generate_comparison_report(self, results_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate comparison report for multiple models"""
        if not results_list:
            return {}
        
        # Aggregate metrics for all models
        comparison_data = {}
        
        for result in results_list:
            model_name = result['model_name']
            metrics = result['metrics']
            
            comparison_data[model_name] = {
                'MRR': metrics['MRR'],
                'NDCG@5': metrics['NDCG@5'],
                'NDCG@10': metrics['NDCG@10'],
                'MAP': metrics['MAP'],
                'ART': metrics['ART'],
                'Precision@1': metrics['Precision@1'],
                'Precision@5': metrics['Precision@5'],
                'Success_Rate': metrics['Success_Rate'],
                'Kendall_Tau': metrics['Kendall_Tau'],
                'Spearman_Rho': metrics['Spearman_Rho']
            }
        
        # Find best models
        best_models = {}
        for metric in ['MRR', 'NDCG@5', 'MAP', 'Precision@1']:
            best_model = max(comparison_data.keys(), 
                           key=lambda x: comparison_data[x][metric])
            best_models[metric] = best_model
        
        # Generate report
        report = {
            'comparison_data': comparison_data,
            'best_models': best_models,
            'total_models': len(results_list),
            'evaluation_date': datetime.now().isoformat(),
            'metrics_definitions': self.metrics_definitions
        }
        
        return report
    
    def save_results(self, results: Dict[str, Any], output_file: str):
        """Save evaluation results"""
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"âœ… Evaluation results saved to: {output_file}")
        except Exception as e:
            logger.error(f"âŒ Failed to save results: {e}")
    
    def evaluate_single_agent_output(self, agent_output: Dict[str, Any], 
                                     ground_truth: Dict[str, Any], 
                                     agent_type: str) -> float:
        """
        Evaluate single agent output for accuracy score
        
        Args:
            agent_output: Agent output
            ground_truth: Corresponding ground truth data
            agent_type: Agent type (for selecting evaluation strategy)
        
        Returns:
            Accuracy score (0.0 - 1.0)
        """
        try:
            # Use different evaluation strategies based on agent type
            if 'safety' in agent_type.lower():
                return self._evaluate_safety_agent(agent_output, ground_truth)
            elif 'economic' in agent_type.lower():
                return self._evaluate_economic_agent(agent_output, ground_truth)
            elif 'weather' in agent_type.lower():
                return self._evaluate_weather_agent(agent_output, ground_truth)
            elif 'flight' in agent_type.lower():
                return self._evaluate_flight_agent(agent_output, ground_truth)
            else:
                # Generic evaluation method
                return self._evaluate_generic_agent(agent_output, ground_truth)
                
        except Exception as e:
            logger.error(f"Single agent evaluation failed {agent_type}: {e}")
            return 0.0
    
    def _evaluate_safety_agent(self, agent_output: Dict[str, Any], 
                              ground_truth: Dict[str, Any]) -> float:
        """Evaluate safety assessment agent"""
        try:
            result = agent_output.get('result', {})
            gt_safety = ground_truth.get('safety_score', 0.0)
            
            if isinstance(result, dict):
                # Extract safety score
                predicted_safety = result.get('overall_safety_score', 
                                            result.get('safety_score', 
                                                     result.get('score', 0.5)))
            else:
                predicted_safety = 0.5
            
            # Calculate accuracy based on closeness to ground truth
            error = abs(predicted_safety - gt_safety)
            accuracy = max(0.0, 1.0 - error)
            
            return accuracy
            
        except Exception as e:
            logger.warning(f"Safety agent evaluation failed: {e}")
            return 0.0
    
    def _evaluate_economic_agent(self, agent_output: Dict[str, Any], 
                                ground_truth: Dict[str, Any]) -> float:
        """Evaluate economic agent"""
        try:
            result = agent_output.get('result', {})
            gt_cost = ground_truth.get('economic_score', 0.0)
            
            if isinstance(result, dict):
                # Extract economic score
                predicted_cost = result.get('total_cost_per_flight', 
                                          result.get('cost_score', 
                                                   result.get('economic_score', 
                                                            result.get('score', 0.5))))
            else:
                predicted_cost = 0.5
            
            # Normalization
            if gt_cost > 0:
                error = abs(predicted_cost - gt_cost) / max(gt_cost, predicted_cost)
                accuracy = max(0.0, 1.0 - error)
            else:
                accuracy = 0.5
            
            return accuracy
            
        except Exception as e:
            logger.warning(f"Economic agent evaluation failed: {e}")
            return 0.0
    
    def _evaluate_weather_agent(self, agent_output: Dict[str, Any], 
                               ground_truth: Dict[str, Any]) -> float:
        """Evaluate weather agent"""
        try:
            result = agent_output.get('result', {})
            gt_weather = ground_truth.get('weather_score', 0.0)
            
            if isinstance(result, dict):
                predicted_weather = result.get('safety_score', 
                                             result.get('weather_score', 
                                                      result.get('score', 0.5)))
            else:
                predicted_weather = 0.5
            
            error = abs(predicted_weather - gt_weather)
            accuracy = max(0.0, 1.0 - error)
            
            return accuracy
            
        except Exception as e:
            logger.warning(f"Weather agent evaluation failed: {e}")
            return 0.0
    
    def _evaluate_flight_agent(self, agent_output: Dict[str, Any], 
                              ground_truth: Dict[str, Any]) -> float:
        """Evaluate flight information agent"""
        try:
            result = agent_output.get('result', {})
            
            # Check if flight information was successfully retrieved
            if isinstance(result, dict) and 'flight_list' in result:
                flight_list = result['flight_list']
                if flight_list and len(flight_list) > 0:
                    # Evaluate based on number of flights retrieved
                    expected_count = ground_truth.get('expected_flight_count', 5)
                    actual_count = len(flight_list)
                    
                    # Calculate coverage
                    coverage = min(1.0, actual_count / expected_count)
                    return coverage
                else:
                    return 0.0
            else:
                return 0.5
                
        except Exception as e:
            logger.warning(f"Flight agent evaluation failed: {e}")
            return 0.0
    
    def _evaluate_generic_agent(self, agent_output: Dict[str, Any], 
                               ground_truth: Dict[str, Any]) -> float:
        """Generic agent evaluation"""
        try:
            # Evaluate based on output completeness and quality
            result = agent_output.get('result', {})
            success = agent_output.get('success', True)
            confidence = agent_output.get('confidence', 0.5)
            
            if not success:
                return 0.0
            
            # If there are specific score fields
            if isinstance(result, dict):
                score = result.get('score', result.get('confidence', confidence))
                return min(1.0, max(0.0, score))
            else:
                return confidence
                
        except Exception as e:
            logger.warning(f"Generic agent evaluation failed: {e}")
            return 0.0
    
    def print_metrics_summary(self, results: Dict[str, Any]):
        """Print evaluation metrics summary"""
        model_name = results['model_name']
        metrics = results['metrics']
        
        print(f"\nðŸ“Š Model {model_name} Evaluation Results:")
        print("=" * 60)
        print(f"ðŸ“ˆ MRR (Mean Reciprocal Rank): {metrics['MRR']:.4f}")
        print(f"ðŸ“ˆ NDCG@5: {metrics['NDCG@5']:.4f}")
        print(f"ðŸ“ˆ NDCG@10: {metrics['NDCG@10']:.4f}")
        print(f"ðŸ“ˆ MAP (Mean Average Precision): {metrics['MAP']:.4f}")
        print(f"ðŸ“ˆ Precision@1: {metrics['Precision@1']:.4f}")
        print(f"ðŸ“ˆ Precision@5: {metrics['Precision@5']:.4f}")
        print(f"â±ï¸  ART (Average Response Time): {metrics['ART']:.4f}s")
        print(f"âœ… Success Rate: {metrics['Success_Rate']:.4f}")
        print(f"ðŸ”— Kendall Tau: {metrics['Kendall_Tau']:.4f}")
        print(f"ðŸ”— Spearman Rho: {metrics['Spearman_Rho']:.4f}")
        print("=" * 60) 

    def perform_statistical_significance_tests(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Perform statistical significance tests between models using results only
        
        Args:
            results: List of model evaluation results from real experiments
            
        Returns:
            Statistical significance test results based on data
        """
        if len(results) < 2:
            logger.warning("Need at least 2 models for statistical significance testing")
            return {'warning': 'Insufficient models for comparison'}
        
        # Extract individual scores from results
        model_names = []
        mrr_scores = {}
        
        for result in results:
            model_name = result.get('model', 'Unknown')
            model_names.append(model_name)
            
            # Extract REAL individual MRR scores only - NO fake generation
            if 'individual_metrics' in result and 'individual_mrr' in result['individual_metrics']:
                # Use individual MRR scores from actual experiments
                mrr_scores[model_name] = result['individual_metrics']['individual_mrr']
                logger.info(f"âœ… Using individual MRR scores for {model_name}: {len(mrr_scores[model_name])} scores")
            else:
                logger.error(f"âŒ INTEGRITY VIOLATION PREVENTED!")
                logger.error(f"   No individual MRR scores found for {model_name}")
                logger.error(f"   Cannot perform without experimental data")
                logger.error(f"   Run complete evaluation with individual score tracking")
                return {
                    'error': 'Missing individual scores',
                    'model': model_name,
                    'required': 'individual_metrics.individual_mrr'
                }
        
        # Verify all models have the same number of test queries
        score_counts = {model: len(scores) for model, scores in mrr_scores.items()}
        if len(set(score_counts.values())) > 1:
            logger.warning(f"Inconsistent number of scores across models: {score_counts}")
        
        # Perform pairwise statistical tests using data only
        test_results = []
        
        for i, model_i in enumerate(model_names):
            for j, model_j in enumerate(model_names):
                if i < j:  # Avoid duplicate tests
                    # Get REAL scores
                    scores_i = mrr_scores[model_i]
                    scores_j = mrr_scores[model_j]
                    
                    # Ensure same number of scores for paired test
                    min_length = min(len(scores_i), len(scores_j))
                    if min_length < 30:
                        logger.warning(f"Small sample size for {model_i} vs {model_j}: {min_length}")
                    
                    # Truncate to same length for valid paired comparison
                    scores_i_paired = scores_i[:min_length]
                    scores_j_paired = scores_j[:min_length]
                    
                    # Perform paired t-test on REAL scores
                    t_stat, p_value = stats.ttest_rel(scores_i_paired, scores_j_paired)
                    
                    # Calculate effect size (Cohen's d)
                    pooled_std = np.sqrt((np.var(scores_i_paired) + np.var(scores_j_paired)) / 2)
                    cohens_d = (np.mean(scores_i_paired) - np.mean(scores_j_paired)) / pooled_std if pooled_std > 0 else 0
                    
                    # Determine effect size category
                    if abs(cohens_d) < 0.2:
                        effect_size = "small"
                    elif abs(cohens_d) < 0.5:
                        effect_size = "medium"
                    else:
                        effect_size = "large"
                    
                    # Determine significance
                    significant = p_value < 0.001  # Conservative threshold
                    
                    test_result = {
                        'model_1': model_i,
                        'model_2': model_j,
                        'comparison': f"{model_i} vs {model_j}",
                        't_statistic': float(t_stat),
                        'p_value': float(p_value),
                        'cohens_d': float(cohens_d),
                        'effect_size': effect_size,
                        'significant': bool(significant),  # Ensure it's Python bool, not numpy bool
                        'sample_size': int(min_length),
                        'mean_1': float(np.mean(scores_i_paired)),
                        'mean_2': float(np.mean(scores_j_paired)),
                        'data_source': 'real_experimental_results'
                    }
                    
                    test_results.append(test_result)
                    
                    logger.info(f"ðŸ“Š Statistical test {model_i} vs {model_j}: "
                               f"p={p_value:.2e}, d={cohens_d:.3f}, n={min_length}")
        
        # Generate summary
        summary = {
            'total_comparisons': len(test_results),
            'significant_comparisons': sum(1 for t in test_results if t['significant']),
            'models_tested': model_names,
            'test_type': 'paired_t_test',
            'significance_level': 0.001,
            'data_integrity': 'real_experimental_data_only'
        }
        
        return {
            'test_results': test_results,
            'summary': summary,
            'academic_integrity': 'verified_real_data_only'
        }
    
    def generate_significance_table(self, significance_results: Dict[str, Any]) -> str:
        """
        Generate a markdown table with statistical significance results
        
        Args:
            significance_results: Statistical test results dictionary
            
        Returns:
            Markdown table as string
        """
        # Check if there's an error in the results
        if 'error' in significance_results:
            error_table = "# Table I: Statistical Significance Analysis - ERROR\n\n"
            error_table += f"**Error**: {significance_results['error']}\n\n"
            error_table += f"**Model**: {significance_results.get('model', 'Unknown')}\n\n"
            error_table += f"**Required**: {significance_results.get('required', 'N/A')}\n\n"
            error_table += "**Action Required**: Run complete evaluation with individual score tracking to enable statistical analysis.\n"
            return error_table
        
        # Check if there's insufficient data
        if 'warning' in significance_results:
            warning_table = "# Table I: Statistical Significance Analysis - WARNING\n\n"
            warning_table += f"**Warning**: {significance_results['warning']}\n\n"
            return warning_table
        
        # Generate normal table from test_results
        test_results = significance_results.get('test_results', [])
        if not test_results:
            return "# Table I: Statistical Significance Analysis - NO DATA\n\nNo statistical test results available.\n"
        
        table = "# Table I: Statistical Significance Analysis of Model Performance via Paired t-test\n\n"
        table += "| Comparison | p-value | Cohen's d | Effect Size | Significant (p < 0.001) |\n"
        table += "|------------|---------|-----------|-------------|-------------------------|\n"
        
        for test in test_results:
            p_value = f"{test['p_value']:.2e}" if test['p_value'] < 0.0001 else f"{test['p_value']:.4f}"
            significant = "Yes" if test['significant'] else "No"
            
            table += f"| {test['comparison']} | {p_value} | {test['cohens_d']:.3f} | {test['effect_size']} | {significant} |\n"
        
        table += "\nNote: All comparisons use paired t-test. Effect sizes: Small (0.2), Medium (0.5), Large (0.8+)."
        
        return table 

    def _calculate_single_query_mrr(self, predicted_ranking: List[str], 
                                   ground_truth_ranking: List[str]) -> float:
        """Calculate MRR for a single query"""
        if not ground_truth_ranking:
            return 0.0
        
        relevant_item = ground_truth_ranking[0]  # Most relevant item
        try:
            rank = predicted_ranking.index(relevant_item) + 1  # 1-indexed
            return 1.0 / rank
        except ValueError:
            return 0.0

    def _calculate_single_query_ndcg(self, predicted_ranking: List[str], 
                                    relevance_scores: Dict[str, float], k: int) -> float:
        """Calculate NDCG@k for a single query"""
        if not predicted_ranking or not relevance_scores:
            return 0.0
        
        # Use relevance scores
        predicted_relevance = []
        for item in predicted_ranking[:k]:
            relevance = relevance_scores.get(item, 0.0)
            predicted_relevance.append(relevance)
        
        if len(predicted_relevance) == 0:
            return 0.0
        
        try:
            # Calculate DCG for predicted ranking
            dcg = self._calculate_dcg(predicted_relevance, k)
            
            # Calculate IDEAL DCG using optimal ranking
            all_relevances = list(relevance_scores.values())
            ideal_relevance = sorted(all_relevances, reverse=True)[:k]
            
            # Pad with zeros if needed
            while len(ideal_relevance) < k:
                ideal_relevance.append(0.0)
            
            idcg = self._calculate_dcg(ideal_relevance, k)
            
            # Calculate NDCG with proper normalization
            if idcg > 0:
                return dcg / idcg
            else:
                return 0.0
                
        except Exception as e:
            logger.warning(f"Single query NDCG calculation failed: {e}")
            return 0.0

    def _calculate_single_query_ap(self, predicted_ranking: List[str], 
                                  relevance_scores: Dict[str, float]) -> float:
        """Calculate Average Precision for a single query"""
        if not predicted_ranking or not relevance_scores:
            return 0.0
        
        # Calculate Average Precision
        relevant_items = []
        precision_at_k = []
        
        for i, item in enumerate(predicted_ranking):
            if relevance_scores.get(item, 0.0) > 0.5:  # Relevance threshold
                relevant_items.append(i + 1)
                precision_at_k.append(len(relevant_items) / (i + 1))
        
        if relevant_items:
            return np.mean(precision_at_k)
        else:
            return 0.0

    def _calculate_single_query_precision_at_k(self, predicted_ranking: List[str], 
                                             ground_truth_ranking: List[str], k: int) -> float:
        """Calculate Precision@k for a single query"""
        if not predicted_ranking or not ground_truth_ranking or k <= 0:
            return 0.0
        
        predicted_at_k = predicted_ranking[:k]
        ground_truth_set = set(ground_truth_ranking)
        
        relevant_count = sum(1 for item in predicted_at_k if item in ground_truth_set)
        return relevant_count / min(k, len(predicted_at_k)) if predicted_at_k else 0.0 